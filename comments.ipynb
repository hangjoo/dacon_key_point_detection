{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "phantom-italy",
   "metadata": {},
   "source": [
    "FAIR(Facebook AI Research)에서 제공하는 Detectron2 라이브러리를 사용하여 전체 학습과 추론 과정을 구현했습니다.<br>\n",
    "사용한 모델은 Detectron2에서 사전 학습된 keypoint-RCNN을 가져와 추가로 학습하여 사용하였고 최종 결과는 Public RMSE < 35인 모델들의 결과를 Majority vote하여 제출하였습니다.<br><br>\n",
    "\n",
    "Detectron2에서 제공하는 Keypoint-RCNN 계열의 경우, COCO 데이터셋의 형식에 맞게 (x, y, v) 좌표를 사용하며, v는 visiable로 아래와 같음<br>\n",
    "0: 이미지에 없음,<br>\n",
    "1: 이미지에 존재하지만 가려져 있음<br>\n",
    "2: 이미지에 존재하고 보이는 상태<br>\n",
    "<br>\n",
    "또한 Detectron2에서 keypoint detection task를 수행할 때 바운딩 박스 좌표도 사용하기 때문에 주어진 키포인트에서 최소 x, y 좌표와 최대 x, y 좌표로 바운딩 박스 영역을 사용했습니다.<br>\n",
    "기본적인 setting에서 적절한 learning rate와 iteration을 찾은 뒤 augmenation에 초점을 맞춰 성능을 향상시키려 시도했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-farming",
   "metadata": {},
   "source": [
    "[학습]\n",
    "1. keypoint_rcnn_X_101_32x8d_FPN_3x(Pretrained)\n",
    "2. Learning Rate: 0.001\n",
    "3. Iteration: 10,000\n",
    "\n",
    "[전처리]\n",
    "1. Albumentations 라이브러리를 사용하여 Crop과 Rotate 위주로 다양하게 전처리하여 사용.\n",
    "2. MS COCO Keypoint Dataset Annotation에 맞게 csv 파일에서 x, y 좌표를 읽어온 뒤 x, y, v로 변환.\n",
    "3. Augmentation을 적용했을 때 특정 키포인트가 잘려 이미지 내에 없을 때 v 값이 0이 되도록 처리.\n",
    "4. Bounding Box 영역도 학습에 사용하기 때문에 키포인트의 최소 x, y 좌표와 최대 x, y 좌표로 바운딩 박스를 넣어줌.\n",
    "\n",
    "[후처리]\n",
    "1. 간혹 추론 과정에서 키포인트가 측정되지 않는 이미지가 존재해 먼저 0으로 채운 뒤, 앙상블 과정에서 다른 모델의 값으로 채워서 사용.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-township",
   "metadata": {},
   "source": [
    "## ENVIRONMENT\n",
    "\n",
    "Detectron2 를 설치할 수 있는 환경 (https://github.com/facebookresearch/detectron2/blob/master/INSTALL.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-washer",
   "metadata": {},
   "source": [
    "## CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-penguin",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "Albumentations 라이브러리를 사용했으며, pytorch에서 제공하는 transform 등과 비교하여 속도가 빠르다고 합니다. 거기에 키포인트도 함께 변환해주는 기능도 있어 사용했습니다.  \n",
    "transform_dict에서 미리 정의해놓고 아래에서 문자열 값을 리스트에 간단히 추가해서 다양하게 augmentation을 적용해서 모델 성능을 측정했습니다.\n",
    "Augmentation된 이미지들은 따로 폴더를 생성하여 저장해놓고 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thrown-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "keypoint_params = A.KeypointParams(format=\"xy\", label_fields=[\"class_labels\"], remove_invisible=False, angle_in_degrees=True)\n",
    "transform_dict = {\n",
    "    \"Original\": A.Compose([A.RandomCrop(height=1080, width=1920, p=1)], keypoint_params=keypoint_params),\n",
    "    \"CenterCrop_1\": A.Compose([A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"CenterCrop_2\": A.Compose([A.CenterCrop(height=960, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_1\": A.Compose([A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_2\": A.Compose([A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_3\": A.Compose([A.RandomCrop(height=960, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_4\": A.Compose([A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_1\": A.Compose([A.RandomCrop(height=960, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_2\": A.Compose([A.RandomCrop(height=720, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_3\": A.Compose([A.RandomCrop(height=540, width=540, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_4\": A.Compose([A.RandomCrop(height=420, width=420, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45\": A.Compose([A.Rotate(limit=45, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_CenterCrop\": A.Compose([A.Rotate(limit=45, p=1), A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_1\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_2\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_3\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=960, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_4\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Random_ScaleCrop\": A.Compose([A.RandomCrop(height=960, width=960, p=1), A.RandomScale(scale_limit=0.35, always_apply=True)], keypoint_params=keypoint_params,),\n",
    "    \"RandomBrightnessContrast\": A.Compose([A.RandomBrightnessContrast(always_apply=True)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_RandomCrop_1\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_RandomCrop_2\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_RandomCrop_3\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_CenterCrop\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_Rotate45_RandomCrop_1\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params,),\n",
    "    \"Rescale_Rotate45_RandomCrop_2\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params,),\n",
    "    \"Rescale_Rotate45_RandomCrop_3\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params,),\n",
    "    \"Rescale_Rotate45_CenterCrop\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params,),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bizarre-colleague",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4195it [30:39,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data\"\n",
    "src_path = os.path.join(data_path, \"original\")\n",
    "src_image_path = os.path.join(src_path, \"train_imgs\")\n",
    "src_df = pd.read_csv(os.path.join(src_path, \"original.csv\"))\n",
    "\n",
    "keypoints_labels = list(map(lambda x: x[:-2], src_df.columns[1:].tolist()[::2]))\n",
    "image_list = src_df.iloc[:, 0].to_numpy()\n",
    "keypoints_list = src_df.iloc[:, 1:].to_numpy()\n",
    "paired_keypoints_list = []\n",
    "for keypoint in keypoints_list:\n",
    "    a_keypoints = []\n",
    "    for i in range(0, keypoint.shape[0], 2):\n",
    "        a_keypoints.append((float(keypoint[i]), float(keypoint[i + 1])))\n",
    "    paired_keypoints_list.append(a_keypoints)\n",
    "paired_keypoints_list = np.array(paired_keypoints_list)\n",
    "\n",
    "dst_name = \"augmented_3\"\n",
    "dst_path = os.path.join(data_path, dst_name)\n",
    "dst_image_path = os.path.join(dst_path, \"train_imgs\")\n",
    "\n",
    "os.makedirs(dst_path, exist_ok=True)\n",
    "os.makedirs(dst_image_path, exist_ok=True)\n",
    "\n",
    "augmented_image_list = []\n",
    "augmented_keypoints_list = []\n",
    "for image_name, paired_keypoints in tqdm(zip(image_list, paired_keypoints_list)):\n",
    "    src_image = cv2.imread(os.path.join(src_image_path, image_name))\n",
    "\n",
    "    transform_names = [\n",
    "        \"Original\",\n",
    "        \"RandomCrop_1\",\n",
    "        \"RandomCrop_2\",\n",
    "        \"RandomSquare_1\",\n",
    "        \"CenterCrop_1\",\n",
    "        \"Rotate45\",\n",
    "        \"Rotate45_CenterCrop\",\n",
    "        \"Rotate45_RandomCrop_1\",\n",
    "        \"Rotate45_RandomCrop_2\",\n",
    "        \"Rotate45_CenterCrop\",\n",
    "        \"Rescale_RandomCrop_1\",\n",
    "        \"Rescale_RandomCrop_2\",\n",
    "        \"Rescale_RandomCrop_3\",\n",
    "        \"Rescale_CenterCrop\",\n",
    "        \"Rescale_Rotate45_RandomCrop_1\",\n",
    "        \"Rescale_Rotate45_RandomCrop_2\",\n",
    "        \"Rescale_Rotate45_RandomCrop_3\",\n",
    "        \"Rescale_Rotate45_CenterCrop\",\n",
    "    ]\n",
    "\n",
    "    for transform_name in transform_names:\n",
    "        augmented = transform_dict[transform_name](\n",
    "            image=src_image, keypoints=paired_keypoints, class_labels=keypoints_labels\n",
    "        )\n",
    "        augmented_image = augmented[\"image\"]\n",
    "        augmented_keypoints = np.array(augmented[\"keypoints\"]).flatten()\n",
    "        augmented_name = f\"{transform_name}_{image_name}\"\n",
    "\n",
    "        cv2.imwrite(os.path.join(dst_image_path, augmented_name), augmented_image)\n",
    "        augmented_image_list.append(augmented_name)\n",
    "        augmented_keypoints_list.append(augmented_keypoints)\n",
    "\n",
    "dst_df = pd.DataFrame(columns=src_df.columns)\n",
    "dst_df[\"image\"] = augmented_image_list\n",
    "dst_df.iloc[:, 1:] = augmented_keypoints_list\n",
    "dst_df.to_csv(os.path.join(dst_path, dst_name + \".csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-cholesterol",
   "metadata": {},
   "source": [
    "### Utils\n",
    "\n",
    "데이터셋을 만들 때 사용되거나 결과를 저장하는 등에 필요해 구현된 함수들을 한 곳에 모아놓고 import 해서 사용했습니다.  \n",
    "\n",
    "**train_val_split**: 학습 데이터셋과 검증 데이터 셋을 나눌 때 사용되며 augmentation된 이미지와 original 이미지들이 섞여있기 때문에 검증 데이터셋에 original 이미지만 포함되도록 구현했습니다.\n",
    "\n",
    "**get_data_dicts**: Detectron2에서 데이터셋을 생성할 때 사용되는 함수입니다. Detectron2의 경우 데이터를 딕셔너리 형태로 제공받아 사용하기 때문에 해당 타입에 맞게 알맞은 키 값을 할당해서 해당 키에 적절한 데이터를 할당하여 return하도록 구현했습니다.\n",
    "\n",
    "**draw_keypoints**와 **save_samples**: 학습이 끝난 모델로 테스트 이미지를 추론할 때 생긴 결과에서 랜덤으로 이미지를 뽑아 키포인트를 그려서 시각화한 뒤 저장하도록 구현된 함수입니다.\n",
    "\n",
    "**fix_random_seed**: random seed를 고정하기 위한 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "attempted-controversy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import HookBase\n",
    "from detectron2.utils.events import get_event_storage\n",
    "\n",
    "\n",
    "def train_val_split(imgs, keypoints, random_state=42):\n",
    "    d = dict()\n",
    "    for file in imgs:\n",
    "        key = ''.join(file.split('-')[:-1])\n",
    "\n",
    "        if key not in d.keys():\n",
    "            d[key] = [file]\n",
    "        else:\n",
    "            d[key].append(file)\n",
    "            \n",
    "    np.random.seed(random_state)\n",
    "    trains = []\n",
    "    validations = []\n",
    "    for key, value in d.items():\n",
    "        r = np.random.randint(len(value), size=2)\n",
    "        for i in range(len(value)):\n",
    "            if \"Origin\" in key and i in r:\n",
    "                validations.append(np.where(imgs == value[i])[0][0])\n",
    "            else:\n",
    "                trains.append(np.where(imgs == value[i])[0][0])\n",
    "    return (\n",
    "        imgs[trains], imgs[validations],\n",
    "        keypoints[trains], keypoints[validations]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_data_dicts(data_dir, imgs, keypoints):\n",
    "    # train_dir = os.path.join(data_dir, \"augmented\" if phase==\"train\" else \"train_imgs\")\n",
    "    train_dir = os.path.join(data_dir, \"train_imgs\")\n",
    "    dataset_dicts = []\n",
    "\n",
    "    for idx, item in tqdm(enumerate(zip(imgs, keypoints))):\n",
    "        img, keypoint = item[0], item[1]\n",
    "\n",
    "        record = {}\n",
    "        filepath = os.path.join(train_dir, img)\n",
    "        record[\"height\"], record[\"width\"] = cv2.imread(filepath).shape[:2]\n",
    "        record[\"file_name\"] = filepath\n",
    "        record[\"image_id\"] = idx\n",
    "\n",
    "        keypoints_v = []\n",
    "        flag = True\n",
    "        for i, keypoint_ in enumerate(keypoint):\n",
    "            keypoints_v.append(keypoint_)  # if coco set, should be added 0.5\n",
    "            if keypoint_ < 0:\n",
    "                flag = False\n",
    "            if i % 2 == 1:\n",
    "                if flag:\n",
    "                    keypoints_v.append(2)\n",
    "                else:\n",
    "                    keypoints_v.append(0)\n",
    "                flag = True\n",
    "\n",
    "        x = keypoint[0::2]\n",
    "        y = keypoint[1::2]\n",
    "        x_min, x_max = min(x), max(x)\n",
    "        y_min, y_max = min(y), max(y)\n",
    "\n",
    "        obj = {\"bbox\": [x_min, y_min, x_max, y_max], \"bbox_mode\": BoxMode.XYXY_ABS, \"category_id\": 0, \"keypoints\": keypoints_v}\n",
    "\n",
    "        record[\"annotations\"] = [obj]\n",
    "        dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "\n",
    "def draw_keypoints(image, keypoints, color=(0, 0, 255), diameter=5):\n",
    "    keypoints_ = keypoints.copy()\n",
    "    if len(keypoints_) == 48:\n",
    "        keypoints_ = [[keypoints_[i], keypoints_[i + 1]] for i in range(0, len(keypoints_), 2)]\n",
    "\n",
    "    assert isinstance(image, np.ndarray), \"image argument does not numpy array.\"\n",
    "    image_ = np.copy(image)\n",
    "    for x, y in keypoints_:\n",
    "        cv2.circle(image_, (int(x), int(y)), diameter, color, -1)\n",
    "\n",
    "    return image_\n",
    "\n",
    "\n",
    "def save_samples(dst_path, image_path, csv_path, mode=\"random\", size=None, index=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if mode == \"random\":\n",
    "        assert size is not None, \"mode argument is random, but size argument is not given.\"\n",
    "        choice_idx = np.random.choice(len(df), size=size, replace=False)\n",
    "    if mode == \"choice\":\n",
    "        assert index is not None, \"mode argument is choice, but index argument is not given.\"\n",
    "        choice_idx = index\n",
    "\n",
    "    for idx in choice_idx:\n",
    "        image_name = df.iloc[idx, 0]\n",
    "        keypoints = df.iloc[idx, 1:]\n",
    "        image = cv2.imread(os.path.join(image_path, image_name), cv2.IMREAD_COLOR)\n",
    "\n",
    "        combined = draw_keypoints(image, keypoints)\n",
    "        cv2.imwrite(os.path.join(dst_path, \"sample\" + image_name), combined)\n",
    "\n",
    "\n",
    "# no use\n",
    "def fix_random_seed(random_seed=42):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-education",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Detectron2에서 사용하는 Trainer로 DefaultTrainer 클래스를 상속받아 COCOEvaluator를 추가하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "needed-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some common detectron2 utilities\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains a number pre-defined logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can use the cleaner\n",
    "    \"SimpleTrainer\", or write your own training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"\n",
    "        Create evaluator(s) for a given dataset.\n",
    "        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "        For your own dataset, you can simply create an evaluator manually in your\n",
    "        script and do not have to worry about the hacky if-else logic here.\n",
    "        \"\"\"\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluator_list = []\n",
    "        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "        if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
    "            evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))\n",
    "        if len(evaluator_list) == 0:\n",
    "            raise NotImplementedError(\"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type))\n",
    "        if len(evaluator_list) == 1:\n",
    "            return evaluator_list[0]\n",
    "        return DatasetEvaluators(evaluator_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-fountain",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "학습이 이뤄지고 마친뒤 추론하는 과정이 담긴 코드입니다.  \n",
    "Detectron2에서 기본으로 horizontal flip transform을 포함하고 있기 때문에 좌우가 뒤집혔을 때 제대로 반영되도록 keypoint_flip_map을 메타데이터에 추가해서 사용했습니다.  \n",
    "Learning Rate는 0.001, iteration은 10000으로 설정하여 학습했습니다.  \n",
    "Coco 키포인트 데이터셋에서는 검증 과정시 스코어 계산을 위해 OKS(Object Keypoint Similarity)를 사용하는데 기존 coco 키포인트에서 사용한 oks sigma 값을 보고 근사해서 넣은 값과 1로 사용했을 때 결과에서 차이가 없어서 1로 사용했습니다.  \n",
    "학습이 끝난 모델로 테스트 이미지를 추론했을 때 간혹 키포인트가 제대로 나오지 않는 이미지가 발생해서 해당 이미지 발생시 0으로 먼저 채워넣고 다른 모델의 추론 값으로 채워넣는 과정을 거쳤습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pharmaceutical-charge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "subsequent-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/08 00:18:10 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (keypoint_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (keypoint_head): KRCNNConvDeconvUpsampleHead(\n",
      "      (conv_fcn1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv_fcn8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (score_lowres): ConvTranspose2d(512, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75000it [13:47, 90.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/08 00:31:58 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 75000 images left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/08 00:31:59 d2.data.build]: \u001b[0mRemoved 1415 images with fewer than 1 keypoints.\n",
      "\u001b[32m[04/08 00:32:00 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   human    | 73585        |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[04/08 00:32:00 d2.data.common]: \u001b[0mSerializing 73585 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/08 00:32:06 d2.data.common]: \u001b[0mSerialized dataset takes 97.65 MiB\n",
      "\u001b[32m[04/08 00:32:06 d2.data.dataset_mapper]: \u001b[0mAugmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[04/08 00:32:06 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.keypoint_head.score_lowres.weight' to the model due to incompatible shapes: (512, 17, 4, 4) in the checkpoint but (512, 24, 4, 4) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.keypoint_head.score_lowres.bias' to the model due to incompatible shapes: (17,) in the checkpoint but (24,) in the model! You might want to double check if this is expected.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/08 00:32:08 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\develop\\projects\\dacon_motion_key_point_detection\\detectron2-windows\\detectron2\\layers\\wrappers.py:226: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  return x.nonzero().unbind(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/08 00:32:48 d2.utils.events]: \u001b[0m eta: 4:03:02  iter: 19  total_loss: 8.422  loss_cls: 0.087  loss_box_reg: 0.127  loss_keypoint: 8.188  loss_rpn_cls: 0.002  loss_rpn_loc: 0.005  time: 1.4341  data_time: 0.5082  lr: 0.000020  max_mem: 6247M\n",
      "\u001b[32m[04/08 00:33:16 d2.utils.events]: \u001b[0m eta: 4:04:44  iter: 39  total_loss: 8.362  loss_cls: 0.060  loss_box_reg: 0.102  loss_keypoint: 8.181  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  time: 1.4307  data_time: 0.0010  lr: 0.000040  max_mem: 6247M\n",
      "\u001b[32m[04/08 00:33:43 d2.utils.events]: \u001b[0m eta: 3:59:55  iter: 59  total_loss: 8.299  loss_cls: 0.064  loss_box_reg: 0.095  loss_keypoint: 8.078  loss_rpn_cls: 0.001  loss_rpn_loc: 0.007  time: 1.3947  data_time: 0.0010  lr: 0.000060  max_mem: 6247M\n",
      "\u001b[32m[04/08 00:34:11 d2.utils.events]: \u001b[0m eta: 4:00:41  iter: 79  total_loss: 8.238  loss_cls: 0.088  loss_box_reg: 0.102  loss_keypoint: 8.004  loss_rpn_cls: 0.002  loss_rpn_loc: 0.006  time: 1.3947  data_time: 0.0010  lr: 0.000080  max_mem: 6247M\n",
      "\u001b[32m[04/08 00:34:39 d2.utils.events]: \u001b[0m eta: 4:00:51  iter: 99  total_loss: 8.087  loss_cls: 0.065  loss_box_reg: 0.074  loss_keypoint: 7.905  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  time: 1.3996  data_time: 0.0009  lr: 0.000100  max_mem: 6247M\n",
      "\u001b[32m[04/08 00:35:07 d2.utils.events]: \u001b[0m eta: 3:59:10  iter: 119  total_loss: 7.908  loss_cls: 0.063  loss_box_reg: 0.079  loss_keypoint: 7.753  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  time: 1.3976  data_time: 0.0010  lr: 0.000120  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:35:35 d2.utils.events]: \u001b[0m eta: 3:59:13  iter: 139  total_loss: 7.870  loss_cls: 0.075  loss_box_reg: 0.088  loss_keypoint: 7.618  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  time: 1.4019  data_time: 0.0010  lr: 0.000140  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:36:04 d2.utils.events]: \u001b[0m eta: 4:00:17  iter: 159  total_loss: 7.442  loss_cls: 0.044  loss_box_reg: 0.064  loss_keypoint: 7.314  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  time: 1.4048  data_time: 0.0010  lr: 0.000160  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:36:32 d2.utils.events]: \u001b[0m eta: 3:58:43  iter: 179  total_loss: 6.996  loss_cls: 0.069  loss_box_reg: 0.104  loss_keypoint: 6.842  loss_rpn_cls: 0.002  loss_rpn_loc: 0.005  time: 1.4039  data_time: 0.0009  lr: 0.000180  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:37:00 d2.utils.events]: \u001b[0m eta: 3:58:14  iter: 199  total_loss: 6.379  loss_cls: 0.079  loss_box_reg: 0.078  loss_keypoint: 6.117  loss_rpn_cls: 0.001  loss_rpn_loc: 0.006  time: 1.4047  data_time: 0.0009  lr: 0.000200  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:37:30 d2.utils.events]: \u001b[0m eta: 3:58:16  iter: 219  total_loss: 5.616  loss_cls: 0.044  loss_box_reg: 0.058  loss_keypoint: 5.495  loss_rpn_cls: 0.001  loss_rpn_loc: 0.004  time: 1.4117  data_time: 0.0009  lr: 0.000220  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:37:58 d2.utils.events]: \u001b[0m eta: 3:57:16  iter: 239  total_loss: 5.170  loss_cls: 0.042  loss_box_reg: 0.058  loss_keypoint: 5.066  loss_rpn_cls: 0.001  loss_rpn_loc: 0.005  time: 1.4114  data_time: 0.0009  lr: 0.000240  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:38:25 d2.utils.events]: \u001b[0m eta: 3:57:18  iter: 259  total_loss: 4.673  loss_cls: 0.037  loss_box_reg: 0.057  loss_keypoint: 4.609  loss_rpn_cls: 0.000  loss_rpn_loc: 0.004  time: 1.4094  data_time: 0.0009  lr: 0.000260  max_mem: 6257M\n",
      "\u001b[32m[04/08 00:38:51 d2.utils.events]: \u001b[0m eta: 3:55:50  iter: 279  total_loss: 4.861  loss_cls: 0.032  loss_box_reg: 0.059  loss_keypoint: 4.709  loss_rpn_cls: 0.000  loss_rpn_loc: 0.005  time: 1.3998  data_time: 0.0009  lr: 0.000280  max_mem: 6257M\n",
      "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[04/08 00:38:58 d2.engine.train_loop]: \u001b[0mException during training:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\develop\\projects\\dacon_motion_key_point_detection\\detectron2-windows\\detectron2\\engine\\train_loop.py\", line 140, in train\n",
      "    self.run_step()\n",
      "  File \"d:\\develop\\projects\\dacon_motion_key_point_detection\\detectron2-windows\\detectron2\\engine\\train_loop.py\", line 234, in run_step\n",
      "    losses.backward()\n",
      "  File \"C:\\Users\\hangjoo\\miniconda3\\envs\\detectron2\\lib\\site-packages\\torch\\tensor.py\", line 185, in backward\n",
      "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
      "  File \"C:\\Users\\hangjoo\\miniconda3\\envs\\detectron2\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 127, in backward\n",
      "    allow_unreachable=True)  # allow_unreachable flag\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 8.00 GiB total capacity; 5.96 GiB already allocated; 0 bytes free; 6.74 GiB reserved in total by PyTorch)\n",
      "Exception raised from malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:272 (most recent call first):\n",
      "00007FFFB10A75A200007FFFB10A7540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n",
      "00007FFFA6B69C0600007FFFA6B69B90 c10_cuda.dll!c10::CUDAOutOfMemoryError::CUDAOutOfMemoryError [<unknown file> @ <unknown line number>]\n",
      "00007FFFA6B7069600007FFFA6B6F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n",
      "00007FFFA6B7083A00007FFFA6B6F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n",
      "00007FFFA6B6509900007FFFA6B64EB0 c10_cuda.dll!c10::cuda::CUDAStream::unpack [<unknown file> @ <unknown line number>]\n",
      "00007FFF18521FF100007FFF18521EB0 torch_cuda.dll!at::native::empty_cuda [<unknown file> @ <unknown line number>]\n",
      "00007FFF18638AFE00007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n",
      "00007FFF186342A500007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FD71A3A00007FFF3FD5D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FD7000500007FFF3FD5D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FE418A000007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FE528DC00007FFF3FE52850 torch_cpu.dll!at::empty [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FC1F32400007FFF3FC1EFB0 torch_cpu.dll!at::TensorIterator::fast_set_up [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FC1D1C400007FFF3FC1D140 torch_cpu.dll!at::TensorIterator::build [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FC1C25D00007FFF3FC1C1A0 torch_cpu.dll!at::TensorIterator::TensorIterator [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FC1D0C600007FFF3FC1D000 torch_cpu.dll!at::TensorIterator::binary_op [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FA825F400007FFF3FA825A0 torch_cpu.dll!at::native::add [<unknown file> @ <unknown line number>]\n",
      "00007FFF18625E8D00007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n",
      "00007FFF186325D400007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FE3EB3F00007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FE2F81A00007FFF3FE2F7A0 torch_cpu.dll!at::add [<unknown file> @ <unknown line number>]\n",
      "00007FFF4116F06E00007FFF410DE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n",
      "00007FFF3F9D1A1400007FFF3F9C6470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FE3EB3F00007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n",
      "00007FFF3FF3D3CA00007FFF3FF3D350 torch_cpu.dll!at::Tensor::add [<unknown file> @ <unknown line number>]\n",
      "00007FFF4158AB3200007FFF41589EF0 torch_cpu.dll!torch::autograd::wrap_outputs [<unknown file> @ <unknown line number>]\n",
      "00007FFF4158B0C900007FFF41589EF0 torch_cpu.dll!torch::autograd::wrap_outputs [<unknown file> @ <unknown line number>]\n",
      "00007FFF41580D0700007FFF4157FFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n",
      "00007FFF41584FE200007FFF41584CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n",
      "00007FFF41584C4100007FFF41584BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n",
      "00007FFEEC2E0A7700007FFEEC2BA150 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n",
      "00007FFF4157BF1400007FFF4157B780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n",
      "00007FFFBE341BB200007FFFBE341B20 ucrtbase.dll!configthreadlocale [<unknown file> @ <unknown line number>]\n",
      "00007FFFC011703400007FFFC0117020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n",
      "00007FFFC0C0265100007FFFC0C02630 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/08 00:38:58 d2.engine.hooks]: \u001b[0mOverall training speed: 283 iterations in 0:06:36 (1.4001 s / it)\n",
      "\u001b[32m[04/08 00:38:58 d2.engine.hooks]: \u001b[0mTotal training time: 0:06:36 (0:00:00 on hooks)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 8.00 GiB total capacity; 5.96 GiB already allocated; 0 bytes free; 6.74 GiB reserved in total by PyTorch)\nException raised from malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:272 (most recent call first):\n00007FFFB10A75A200007FFFB10A7540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFFA6B69C0600007FFFA6B69B90 c10_cuda.dll!c10::CUDAOutOfMemoryError::CUDAOutOfMemoryError [<unknown file> @ <unknown line number>]\n00007FFFA6B7069600007FFFA6B6F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFFA6B7083A00007FFFA6B6F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFFA6B6509900007FFFA6B64EB0 c10_cuda.dll!c10::cuda::CUDAStream::unpack [<unknown file> @ <unknown line number>]\n00007FFF18521FF100007FFF18521EB0 torch_cuda.dll!at::native::empty_cuda [<unknown file> @ <unknown line number>]\n00007FFF18638AFE00007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF186342A500007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF3FD71A3A00007FFF3FD5D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFF3FD7000500007FFF3FD5D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFF3FE418A000007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFF3FE528DC00007FFF3FE52850 torch_cpu.dll!at::empty [<unknown file> @ <unknown line number>]\n00007FFF3FC1F32400007FFF3FC1EFB0 torch_cpu.dll!at::TensorIterator::fast_set_up [<unknown file> @ <unknown line number>]\n00007FFF3FC1D1C400007FFF3FC1D140 torch_cpu.dll!at::TensorIterator::build [<unknown file> @ <unknown line number>]\n00007FFF3FC1C25D00007FFF3FC1C1A0 torch_cpu.dll!at::TensorIterator::TensorIterator [<unknown file> @ <unknown line number>]\n00007FFF3FC1D0C600007FFF3FC1D000 torch_cpu.dll!at::TensorIterator::binary_op [<unknown file> @ <unknown line number>]\n00007FFF3FA825F400007FFF3FA825A0 torch_cpu.dll!at::native::add [<unknown file> @ <unknown line number>]\n00007FFF18625E8D00007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF186325D400007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF3FE3EB3F00007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFF3FE2F81A00007FFF3FE2F7A0 torch_cpu.dll!at::add [<unknown file> @ <unknown line number>]\n00007FFF4116F06E00007FFF410DE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFF3F9D1A1400007FFF3F9C6470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFF3FE3EB3F00007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFF3FF3D3CA00007FFF3FF3D350 torch_cpu.dll!at::Tensor::add [<unknown file> @ <unknown line number>]\n00007FFF4158AB3200007FFF41589EF0 torch_cpu.dll!torch::autograd::wrap_outputs [<unknown file> @ <unknown line number>]\n00007FFF4158B0C900007FFF41589EF0 torch_cpu.dll!torch::autograd::wrap_outputs [<unknown file> @ <unknown line number>]\n00007FFF41580D0700007FFF4157FFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FFF41584FE200007FFF41584CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FFF41584C4100007FFF41584BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FFEEC2E0A7700007FFEEC2BA150 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFF4157BF1400007FFF4157B780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FFFBE341BB200007FFFBE341B20 ucrtbase.dll!configthreadlocale [<unknown file> @ <unknown line number>]\n00007FFFC011703400007FFFC0117020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FFFC0C0265100007FFFC0C02630 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-bfe1955612aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;31m# Inference should use the config with parameters that are used in training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\develop\\projects\\dacon_motion_key_point_detection\\detectron2-windows\\detectron2\\engine\\defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \"\"\"\n\u001b[1;32m--> 400\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             assert hasattr(\n",
      "\u001b[1;32md:\\develop\\projects\\dacon_motion_key_point_detection\\detectron2-windows\\detectron2\\engine\\train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\develop\\projects\\dacon_motion_key_point_detection\\detectron2-windows\\detectron2\\engine\\train_loop.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \"\"\"\n\u001b[0;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;31m# use a new stream so the ops don't wait for DDP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\detectron2\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\detectron2\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 8.00 GiB total capacity; 5.96 GiB already allocated; 0 bytes free; 6.74 GiB reserved in total by PyTorch)\nException raised from malloc at ..\\c10\\cuda\\CUDACachingAllocator.cpp:272 (most recent call first):\n00007FFFB10A75A200007FFFB10A7540 c10.dll!c10::Error::Error [<unknown file> @ <unknown line number>]\n00007FFFA6B69C0600007FFFA6B69B90 c10_cuda.dll!c10::CUDAOutOfMemoryError::CUDAOutOfMemoryError [<unknown file> @ <unknown line number>]\n00007FFFA6B7069600007FFFA6B6F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFFA6B7083A00007FFFA6B6F370 c10_cuda.dll!c10::cuda::CUDACachingAllocator::init [<unknown file> @ <unknown line number>]\n00007FFFA6B6509900007FFFA6B64EB0 c10_cuda.dll!c10::cuda::CUDAStream::unpack [<unknown file> @ <unknown line number>]\n00007FFF18521FF100007FFF18521EB0 torch_cuda.dll!at::native::empty_cuda [<unknown file> @ <unknown line number>]\n00007FFF18638AFE00007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF186342A500007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF3FD71A3A00007FFF3FD5D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFF3FD7000500007FFF3FD5D9D0 torch_cpu.dll!at::native::mkldnn_sigmoid_ [<unknown file> @ <unknown line number>]\n00007FFF3FE418A000007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFF3FE528DC00007FFF3FE52850 torch_cpu.dll!at::empty [<unknown file> @ <unknown line number>]\n00007FFF3FC1F32400007FFF3FC1EFB0 torch_cpu.dll!at::TensorIterator::fast_set_up [<unknown file> @ <unknown line number>]\n00007FFF3FC1D1C400007FFF3FC1D140 torch_cpu.dll!at::TensorIterator::build [<unknown file> @ <unknown line number>]\n00007FFF3FC1C25D00007FFF3FC1C1A0 torch_cpu.dll!at::TensorIterator::TensorIterator [<unknown file> @ <unknown line number>]\n00007FFF3FC1D0C600007FFF3FC1D000 torch_cpu.dll!at::TensorIterator::binary_op [<unknown file> @ <unknown line number>]\n00007FFF3FA825F400007FFF3FA825A0 torch_cpu.dll!at::native::add [<unknown file> @ <unknown line number>]\n00007FFF18625E8D00007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF186325D400007FFF185DE0A0 torch_cuda.dll!at::native::set_storage_cuda_ [<unknown file> @ <unknown line number>]\n00007FFF3FE3EB3F00007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFF3FE2F81A00007FFF3FE2F7A0 torch_cpu.dll!at::add [<unknown file> @ <unknown line number>]\n00007FFF4116F06E00007FFF410DE010 torch_cpu.dll!torch::autograd::GraphRoot::apply [<unknown file> @ <unknown line number>]\n00007FFF3F9D1A1400007FFF3F9C6470 torch_cpu.dll!torch::nn::functional::BatchNormFuncOptions::~BatchNormFuncOptions [<unknown file> @ <unknown line number>]\n00007FFF3FE3EB3F00007FFF3FE38FA0 torch_cpu.dll!at::bucketize_out [<unknown file> @ <unknown line number>]\n00007FFF3FF3D3CA00007FFF3FF3D350 torch_cpu.dll!at::Tensor::add [<unknown file> @ <unknown line number>]\n00007FFF4158AB3200007FFF41589EF0 torch_cpu.dll!torch::autograd::wrap_outputs [<unknown file> @ <unknown line number>]\n00007FFF4158B0C900007FFF41589EF0 torch_cpu.dll!torch::autograd::wrap_outputs [<unknown file> @ <unknown line number>]\n00007FFF41580D0700007FFF4157FFD0 torch_cpu.dll!torch::autograd::Engine::evaluate_function [<unknown file> @ <unknown line number>]\n00007FFF41584FE200007FFF41584CA0 torch_cpu.dll!torch::autograd::Engine::thread_main [<unknown file> @ <unknown line number>]\n00007FFF41584C4100007FFF41584BC0 torch_cpu.dll!torch::autograd::Engine::thread_init [<unknown file> @ <unknown line number>]\n00007FFEEC2E0A7700007FFEEC2BA150 torch_python.dll!THPShortStorage_New [<unknown file> @ <unknown line number>]\n00007FFF4157BF1400007FFF4157B780 torch_cpu.dll!torch::autograd::Engine::get_base_engine [<unknown file> @ <unknown line number>]\n00007FFFBE341BB200007FFFBE341B20 ucrtbase.dll!configthreadlocale [<unknown file> @ <unknown line number>]\n00007FFFC011703400007FFFC0117020 KERNEL32.DLL!BaseThreadInitThunk [<unknown file> @ <unknown line number>]\n00007FFFC0C0265100007FFFC0C02630 ntdll.dll!RtlUserThreadStart [<unknown file> @ <unknown line number>]\n"
     ]
    }
   ],
   "source": [
    "experiment_id = 3\n",
    "data_name = \"augmented_3\"\n",
    "data_path = os.path.join(\"./data\", data_name)\n",
    "csv_name = data_name + \".csv\"\n",
    "train_df = pd.read_csv(os.path.join(data_path, csv_name))\n",
    "\n",
    "keypoint_names = list(map(lambda x: x[:-2], train_df.columns.to_list()[1::2]))\n",
    "keypoint_flip_map = [\n",
    "    (\"left_eye\", \"right_eye\"),\n",
    "    (\"left_ear\", \"right_ear\"),\n",
    "    (\"left_shoulder\", \"right_shoulder\"),\n",
    "    (\"left_elbow\", \"right_elbow\"),\n",
    "    (\"left_wrist\", \"right_wrist\"),\n",
    "    (\"left_hip\", \"right_hip\"),\n",
    "    (\"left_knee\", \"right_knee\"),\n",
    "    (\"left_ankle\", \"right_ankle\"),\n",
    "    (\"left_palm\", \"right_palm\"),\n",
    "    (\"left_instep\", \"right_instep\"),\n",
    "]\n",
    "\n",
    "image_list = train_df.iloc[:, 0].to_numpy()\n",
    "keypoints_list = train_df.iloc[:, 1:].to_numpy()\n",
    "train_imgs, valid_imgs, train_keypoints, valid_keypoints = train_val_split(image_list, keypoints_list, random_state=42)\n",
    "\n",
    "image_set = {\"train\": train_imgs, \"valid\": valid_imgs}\n",
    "keypoints_set = {\"train\": train_keypoints, \"valid\": valid_keypoints}\n",
    "\n",
    "hyper_params = {\n",
    "    \"augmented_ver\": data_name,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 10000,\n",
    "    \"batch_size\": 256,\n",
    "    \"description\": \"Final training\"\n",
    "}\n",
    "\n",
    "for phase in [\"train\", \"valid\"]:\n",
    "    DatasetCatalog.register(\n",
    "        \"keypoints_\" + phase, lambda phase=phase: get_data_dicts(data_path, image_set[phase], keypoints_set[phase])\n",
    "    )\n",
    "    MetadataCatalog.get(\"keypoints_\" + phase).set(thing_classes=[\"human\"])\n",
    "    MetadataCatalog.get(\"keypoints_\" + phase).set(keypoint_names=keypoint_names)\n",
    "    MetadataCatalog.get(\"keypoints_\" + phase).set(keypoint_flip_map=keypoint_flip_map)\n",
    "    MetadataCatalog.get(\"keypoints_\" + phase).set(evaluator_type=\"coco\")\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (\"keypoints_train\",)\n",
    "cfg.DATASETS.TEST = (\"keypoints_valid\",)\n",
    "cfg.DATALOADER.NUM_WORKERS = 16  # On Windows environment, this value must be 0.\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2  # mini batch size would be (SOLVER.IMS_PER_BATCH) * (ROI_HEADS.BATCH_SIZE_PER_IMAGE).\n",
    "cfg.SOLVER.BASE_LR = hyper_params[\"learning_rate\"]  # Learning Rate.\n",
    "cfg.SOLVER.MAX_ITER = hyper_params[\"num_epochs\"]  # Max iteration.\n",
    "cfg.SOLVER.GAMMA = 0.8\n",
    "cfg.SOLVER.STEPS = [3000, 4000, 5000, 6000, 7000, 8000]  # The iteration number to decrease learning rate by GAMMA.\n",
    "# cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = hyper_params[\"batch_size\"]  # Use to calculate RPN loss.\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 24\n",
    "cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((24, 1), dtype=float).tolist()\n",
    "cfg.TEST.EVAL_PERIOD = 5000  # Evaluation would occur for every cfg.TEST.EVAL_PERIOD value.\n",
    "cfg.OUTPUT_DIR = os.path.join(\"./output\", f\"DAC-{experiment_id}\")\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "# Inference should use the config with parameters that are used in training\n",
    "# cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set a custom testing threshold\n",
    "predictor = DefaultPredictor(cfg)\n",
    "\n",
    "test_dir = os.path.join(\"data\", \"test_imgs\")\n",
    "test_list = os.listdir(test_dir)\n",
    "test_list.sort()\n",
    "except_list = []\n",
    "\n",
    "files = []\n",
    "preds = []\n",
    "for file in tqdm(test_list):\n",
    "    filepath = os.path.join(test_dir, file)\n",
    "    # print(filepath)\n",
    "    im = cv2.imread(filepath)\n",
    "    outputs = predictor(im)\n",
    "    outputs = outputs[\"instances\"].to(\"cpu\").get(\"pred_keypoints\").numpy()\n",
    "    files.append(file)\n",
    "    pred = []\n",
    "    try:\n",
    "        for out in outputs[0]:\n",
    "            pred.extend([float(e) for e in out[:2]])\n",
    "    except IndexError:\n",
    "        pred.extend([0] * 48)\n",
    "        except_list.append(filepath)\n",
    "    preds.append(pred)\n",
    "\n",
    "df_sub = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "df = pd.DataFrame(columns=df_sub.columns)\n",
    "df[\"image\"] = files\n",
    "df.iloc[:, 1:] = preds\n",
    "\n",
    "df.to_csv(os.path.join(cfg.OUTPUT_DIR, f\"DAC-{experiment_id}.csv\"), index=False)\n",
    "if except_list:\n",
    "    print(\n",
    "        \"The following images are not detected keypoints. The row corresponding that images names would be filled with 0 value.\"\n",
    "    )\n",
    "    print(*except_list)\n",
    "save_samples(cfg.OUTPUT_DIR, test_dir, os.path.join(cfg.OUTPUT_DIR, f\"{data_name}_submission.csv\"), mode=\"random\", size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-assembly",
   "metadata": {},
   "source": [
    "### Majority votes\n",
    "\n",
    "최종 결과들을 public score에 따라 weighted average 하여 성능을 향상시켰습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "results = [] # 각 제출별 public score\n",
    "file_indexs = [] # 제출 파일명\n",
    "s = sum(results)\n",
    "\n",
    "for result, i in zip(results, file_indexs):\n",
    "    df_ = pd.read_csv(f\"./output/DAC-{i}/DAC-{i}.csv\")\n",
    "    weight = (2 / len(results)) - (result / s)\n",
    "    df_.iloc[:, 1:] *= weight\n",
    "    df.append(df_)\n",
    "df = pd.concat(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = df.iloc[:1600]['image']\n",
    "l = []\n",
    "for img in imgs:\n",
    "    r = [img] + list(df[df['image'] == img].iloc[:, 1:].sum())\n",
    "    l.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_votes = pd.DataFrame(columns=df.columns, data=l)\n",
    "df_votes.to_csv(f'./weighted_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
