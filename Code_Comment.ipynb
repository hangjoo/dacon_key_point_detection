{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "treated-groove",
   "metadata": {},
   "source": [
    "FAIR(Facebook AI Research)에서 제공하는 Detectron2 라이브러리를 사용하여 전체 학습과 추론 과정을 구현했습니다.  \n",
    "사용한 모델은 Detectron2에서 사전 학습된 keypoint-RCNN을 가져와 추가로 학습하여 사용하였으며,  \n",
    "최종 결과는 이렇게 생성된 여러 모델 중 Public 점수가 34점대인 모델을 앙상블해서 제출하였습니다.  \n",
    "\n",
    "Coco 키포인트 데이터셋 형식이 키포인트의 x 좌표와 y 좌표에 추가로 이미지 내에 보여지는지 visible 값도 사용하기 때문에, augmentation을 적용했을 때 키포인트가 잘리는 경우 visible 값이 0이 되도록 변환하여 사용했습니다. 추가로 Detectron2에서 keypoint detection task를 수행할 때 바운딩 박스 좌표도 사용하기 때문에 주어진 키포인트에서 최소 x, y 좌표와 최대 x, y 좌표로 바운딩 박스 영역을 잡아 사용했습니다.  \n",
    "\n",
    "여기에 오리지널 데이터셋을 적용했을 때 점수가 상당히 잘 나와서 적절한 learning rate와 iteration을 찾은 뒤 augmenation에 초점을 맞춰 성능을 올렸습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-classification",
   "metadata": {},
   "source": [
    "[학습]\n",
    "1. keypoint_rcnn_X_101_32x8d_FPN_3x(Pretrained)\n",
    "2. Learning Rate: 0.001\n",
    "3. Iteration: 10,000\n",
    "\n",
    "[전처리]\n",
    "1. Albumentations 라이브러리를 사용하여 Crop과 Rotate 위주로 다양하게 전처리하여 사용.\n",
    "2. CoCo keypoint Dataset Annotation에 맞게 csv 파일에서 x, y 좌표를 읽어온 뒤 x, y, v로 변환.\n",
    "3. Augmentation을 적용했을 때 특정 키포인트가 잘려 이미지 내에 없을 때 v 값이 0이 되도록 처리.\n",
    "4. Bounding Box 영역도 학습에 사용하기 때문에 키포인트의 최소 x, y 좌표와 최대 x, y 좌표로 바운딩 박스를 넣어줌.\n",
    "\n",
    "[후처리]\n",
    "1. 간혹 추론 과정에서 키포인트가 측정되지 않는 이미지가 존재해 먼저 0으로 채운 뒤, 앙상블 과정에서 다른 모델의 값으로 채워서 사용.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-suffering",
   "metadata": {},
   "source": [
    "## 환경\n",
    "\n",
    "Detectron2의 경우 현재 windows os의 경우 정식 지원하지 않아, windows 환경에 맞게 수정된 깃헙 레포지토리를 클론하여 사용하거나 리눅스 환경에서 사용해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-mississippi",
   "metadata": {},
   "source": [
    "## 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-burns",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "Albumentations 라이브러리를 사용했으며, pytorch에서 제공하는 transform 등과 비교하여 속도가 빠르다고 합니다. 거기에 키포인트도 함께 변환해주는 기능도 있어 사용했습니다.  \n",
    "transform_dict에서 미리 정의해놓고 아래에서 문자열 값을 리스트에 간단히 추가해서 다양하게 augmentation을 적용해서 모델 성능을 측정했습니다.\n",
    "Augmentation된 이미지들은 따로 폴더를 생성하여 저장해놓고 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-progressive",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3250it [17:30,  3.06it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "keypoint_params = A.KeypointParams(format=\"xy\", label_fields=[\"class_labels\"], remove_invisible=False, angle_in_degrees=True)\n",
    "transform_dict = {\n",
    "    \"Original\": A.Compose([A.RandomCrop(height=1080, width=1920, p=1)], keypoint_params=keypoint_params),\n",
    "    \"CenterCrop_1\": A.Compose([A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"CenterCrop_2\": A.Compose([A.CenterCrop(height=960, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_1\": A.Compose([A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_2\": A.Compose([A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_3\": A.Compose([A.RandomCrop(height=960, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomCrop_4\": A.Compose([A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_1\": A.Compose([A.RandomCrop(height=960, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_2\": A.Compose([A.RandomCrop(height=720, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_3\": A.Compose([A.RandomCrop(height=540, width=540, p=1)], keypoint_params=keypoint_params),\n",
    "    \"RandomSquare_4\": A.Compose([A.RandomCrop(height=420, width=420, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45\": A.Compose([A.Rotate(limit=45, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_CenterCrop\": A.Compose([A.Rotate(limit=45, p=1), A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_1\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_2\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_3\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=960, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rotate45_RandomCrop_4\": A.Compose([A.Rotate(limit=45, p=1), A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Random_ScaleCrop\": A.Compose([A.RandomCrop(height=960, width=960, p=1), A.RandomScale(scale_limit=0.35, always_apply=True)], keypoint_params=keypoint_params,),\n",
    "    \"RandomBrightnessContrast\": A.Compose([A.RandomBrightnessContrast(always_apply=True)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_RandomCrop_1\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_RandomCrop_2\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_RandomCrop_3\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_CenterCrop\": A.Compose([A.RandomScale(scale_limit=0.3, p=1), A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params),\n",
    "    \"Rescale_Rotate45_RandomCrop_1\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=960, p=1)], keypoint_params=keypoint_params,),\n",
    "    \"Rescale_Rotate45_RandomCrop_2\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=540, width=720, p=1)], keypoint_params=keypoint_params,),\n",
    "    \"Rescale_Rotate45_RandomCrop_3\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.RandomCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params,),\n",
    "    \"Rescale_Rotate45_CenterCrop\": A.Compose([A.Rotate(limit=45, p=1), A.RandomScale(scale_limit=0.3, p=1), A.CenterCrop(height=720, width=1280, p=1)], keypoint_params=keypoint_params,),\n",
    "}\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_path = \"./data\"\n",
    "    src_path = os.path.join(data_path, \"original\")\n",
    "    src_image_path = os.path.join(src_path, \"train_imgs\")\n",
    "    src_df = pd.read_csv(os.path.join(src_path, \"original.csv\"))\n",
    "\n",
    "    keypoints_labels = list(map(lambda x: x[:-2], src_df.columns[1:].tolist()[::2]))\n",
    "    image_list = src_df.iloc[:, 0].to_numpy()\n",
    "    keypoints_list = src_df.iloc[:, 1:].to_numpy()\n",
    "    paired_keypoints_list = []\n",
    "    for keypoint in keypoints_list:\n",
    "        a_keypoints = []\n",
    "        for i in range(0, keypoint.shape[0], 2):\n",
    "            a_keypoints.append((float(keypoint[i]), float(keypoint[i + 1])))\n",
    "        paired_keypoints_list.append(a_keypoints)\n",
    "    paired_keypoints_list = np.array(paired_keypoints_list)\n",
    "\n",
    "    dst_name = \"augmented_3\"\n",
    "    dst_path = os.path.join(data_path, dst_name)\n",
    "    dst_image_path = os.path.join(dst_path, \"train_imgs\")\n",
    "\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "    os.makedirs(dst_image_path, exist_ok=True)\n",
    "\n",
    "    augmented_image_list = []\n",
    "    augmented_keypoints_list = []\n",
    "    for image_name, paired_keypoints in tqdm(zip(image_list, paired_keypoints_list)):\n",
    "        src_image = cv2.imread(os.path.join(src_image_path, image_name))\n",
    "\n",
    "        transform_names = [\n",
    "            \"Original\",\n",
    "            \"RandomCrop_1\",\n",
    "            \"RandomCrop_2\",\n",
    "            \"RandomSquare_1\",\n",
    "            \"CenterCrop_1\",\n",
    "            \"Rotate45\",\n",
    "            \"Rotate45_CenterCrop\",\n",
    "            \"Rotate45_RandomCrop_1\",\n",
    "            \"Rotate45_RandomCrop_2\",\n",
    "            \"Rotate45_CenterCrop\",\n",
    "            \"Rescale_RandomCrop_1\",\n",
    "            \"Rescale_RandomCrop_2\",\n",
    "            \"Rescale_RandomCrop_3\",\n",
    "            \"Rescale_CenterCrop\",\n",
    "            \"Rescale_Rotate45_RandomCrop_1\",\n",
    "            \"Rescale_Rotate45_RandomCrop_2\",\n",
    "            \"Rescale_Rotate45_RandomCrop_3\",\n",
    "            \"Rescale_Rotate45_CenterCrop\",\n",
    "        ]\n",
    "\n",
    "        for transform_name in transform_names:\n",
    "            augmented = transform_dict[transform_name](\n",
    "                image=src_image, keypoints=paired_keypoints, class_labels=keypoints_labels\n",
    "            )\n",
    "            augmented_image = augmented[\"image\"]\n",
    "            augmented_keypoints = np.array(augmented[\"keypoints\"]).flatten()\n",
    "            augmented_name = f\"{transform_name}_{image_name}\"\n",
    "\n",
    "            cv2.imwrite(os.path.join(dst_image_path, augmented_name), augmented_image)\n",
    "            augmented_image_list.append(augmented_name)\n",
    "            augmented_keypoints_list.append(augmented_keypoints)\n",
    "\n",
    "    dst_df = pd.DataFrame(columns=src_df.columns)\n",
    "    dst_df[\"image\"] = augmented_image_list\n",
    "    dst_df.iloc[:, 1:] = augmented_keypoints_list\n",
    "    dst_df.to_csv(os.path.join(dst_path, dst_name + \".csv\"), index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-duration",
   "metadata": {},
   "source": [
    "### Utils\n",
    "\n",
    "데이터셋을 만들 때 사용되거나 결과를 저장하는 등에 필요해 구현된 함수들을 한 곳에 모아놓고 import 해서 사용했습니다.  \n",
    "\n",
    "**train_val_split**: 학습 데이터셋과 검증 데이터 셋을 나눌 때 사용되며 augmentation된 이미지와 original 이미지들이 섞여있기 때문에 검증 데이터셋에 original 이미지만 포함되도록 구현했습니다.\n",
    "\n",
    "**get_data_dicts**: Detectron2에서 데이터셋을 생성할 때 사용되는 함수입니다. Detectron2의 경우 데이터를 딕셔너리 형태로 제공받아 사용하기 때문에 해당 타입에 맞게 알맞은 키 값을 할당해서 해당 키에 적절한 데이터를 할당하여 return하도록 구현했습니다.\n",
    "\n",
    "**draw_keypoints**와 **save_samples**: 학습이 끝난 모델로 테스트 이미지를 추론할 때 생긴 결과에서 랜덤으로 이미지를 뽑아 키포인트를 그려서 시각화한 뒤 저장하도록 구현된 함수입니다.\n",
    "\n",
    "**fix_random_seed**: random seed를 고정하기 위한 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "\n",
    "import neptune\n",
    "\n",
    "import torch\n",
    "\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import HookBase\n",
    "from detectron2.utils.events import get_event_storage\n",
    "\n",
    "\n",
    "def train_val_split(imgs, keypoints, random_state=42):\n",
    "    d = dict()\n",
    "    for file in imgs:\n",
    "        key = ''.join(file.split('-')[:-1])\n",
    "\n",
    "        if key not in d.keys():\n",
    "            d[key] = [file]\n",
    "        else:\n",
    "            d[key].append(file)\n",
    "            \n",
    "    np.random.seed(random_state)\n",
    "    trains = []\n",
    "    validations = []\n",
    "    for key, value in d.items():\n",
    "        r = np.random.randint(len(value), size=2)\n",
    "        for i in range(len(value)):\n",
    "            if \"Origin\" in key and i in r:\n",
    "                validations.append(np.where(imgs == value[i])[0][0])\n",
    "            else:\n",
    "                trains.append(np.where(imgs == value[i])[0][0])\n",
    "    return (\n",
    "        imgs[trains], imgs[validations],\n",
    "        keypoints[trains], keypoints[validations]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_data_dicts(data_dir, imgs, keypoints):\n",
    "    # train_dir = os.path.join(data_dir, \"augmented\" if phase==\"train\" else \"train_imgs\")\n",
    "    train_dir = os.path.join(data_dir, \"train_imgs\")\n",
    "    dataset_dicts = []\n",
    "\n",
    "    for idx, item in tqdm(enumerate(zip(imgs, keypoints))):\n",
    "        img, keypoint = item[0], item[1]\n",
    "\n",
    "        record = {}\n",
    "        filepath = os.path.join(train_dir, img)\n",
    "        record[\"height\"], record[\"width\"] = cv2.imread(filepath).shape[:2]\n",
    "        record[\"file_name\"] = filepath\n",
    "        record[\"image_id\"] = idx\n",
    "\n",
    "        keypoints_v = []\n",
    "        flag = True\n",
    "        for i, keypoint_ in enumerate(keypoint):\n",
    "            keypoints_v.append(keypoint_)  # if coco set, should be added 0.5\n",
    "            if keypoint_ < 0:\n",
    "                flag = False\n",
    "            if i % 2 == 1:\n",
    "                if flag:\n",
    "                    keypoints_v.append(2)\n",
    "                else:\n",
    "                    keypoints_v.append(0)\n",
    "                flag = True\n",
    "\n",
    "        x = keypoint[0::2]\n",
    "        y = keypoint[1::2]\n",
    "        x_min, x_max = min(x), max(x)\n",
    "        y_min, y_max = min(y), max(y)\n",
    "\n",
    "        obj = {\"bbox\": [x_min, y_min, x_max, y_max], \"bbox_mode\": BoxMode.XYXY_ABS, \"category_id\": 0, \"keypoints\": keypoints_v}\n",
    "\n",
    "        record[\"annotations\"] = [obj]\n",
    "        dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "\n",
    "def draw_keypoints(image, keypoints, color=(0, 0, 255), diameter=5):\n",
    "    keypoints_ = keypoints.copy()\n",
    "    if len(keypoints_) == 48:\n",
    "        keypoints_ = [[keypoints_[i], keypoints_[i + 1]] for i in range(0, len(keypoints_), 2)]\n",
    "\n",
    "    assert isinstance(image, np.ndarray), \"image argument does not numpy array.\"\n",
    "    image_ = np.copy(image)\n",
    "    for x, y in keypoints_:\n",
    "        cv2.circle(image_, (int(x), int(y)), diameter, color, -1)\n",
    "\n",
    "    return image_\n",
    "\n",
    "\n",
    "def save_samples(dst_path, image_path, csv_path, mode=\"random\", size=None, index=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    if mode == \"random\":\n",
    "        assert size is not None, \"mode argument is random, but size argument is not given.\"\n",
    "        choice_idx = np.random.choice(len(df), size=size, replace=False)\n",
    "    if mode == \"choice\":\n",
    "        assert index is not None, \"mode argument is choice, but index argument is not given.\"\n",
    "        choice_idx = index\n",
    "\n",
    "    for idx in choice_idx:\n",
    "        image_name = df.iloc[idx, 0]\n",
    "        keypoints = df.iloc[idx, 1:]\n",
    "        image = cv2.imread(os.path.join(image_path, image_name), cv2.IMREAD_COLOR)\n",
    "\n",
    "        combined = draw_keypoints(image, keypoints)\n",
    "        cv2.imwrite(os.path.join(dst_path, \"sample\" + image_name), combined)\n",
    "\n",
    "\n",
    "def fix_random_seed(random_seed=423):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coral-diana",
   "metadata": {},
   "source": [
    "### Trainer\n",
    "\n",
    "Detectron2에서 사용하는 Trainer로 DefaultTrainer 클래스를 상속받아 변경해서 사용했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "# import some common libraries\n",
    "import os\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "\n",
    "setup_logger()\n",
    "\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains a number pre-defined logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can use the cleaner\n",
    "    \"SimpleTrainer\", or write your own training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        \"\"\"\n",
    "        Create evaluator(s) for a given dataset.\n",
    "        This uses the special metadata \"evaluator_type\" associated with each builtin dataset.\n",
    "        For your own dataset, you can simply create an evaluator manually in your\n",
    "        script and do not have to worry about the hacky if-else logic here.\n",
    "        \"\"\"\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        evaluator_list = []\n",
    "        evaluator_type = MetadataCatalog.get(dataset_name).evaluator_type\n",
    "        if evaluator_type in [\"coco\", \"coco_panoptic_seg\"]:\n",
    "            evaluator_list.append(COCOEvaluator(dataset_name, cfg, True, output_folder))\n",
    "        if len(evaluator_list) == 0:\n",
    "            raise NotImplementedError(\"no Evaluator for the dataset {} with the type {}\".format(dataset_name, evaluator_type))\n",
    "        if len(evaluator_list) == 1:\n",
    "            return evaluator_list[0]\n",
    "        return DatasetEvaluators(evaluator_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-chemical",
   "metadata": {},
   "source": [
    "### Train\n",
    "\n",
    "학습이 이뤄지고 마친뒤 추론하는 과정이 담긴 코드입니다.  \n",
    "Detectron2에서 기본으로 horizontal flip transform을 포함하고 있기 때문에 좌우가 뒤집혔을 때 제대로 반영되도록 keypoint_flip_map을 메타데이터에 추가해서 사용했습니다. \n",
    "Learning Rate는 0.001, iteration은 10000으로 설정하여 학습했습니다.\n",
    "Coco 키포인트 데이터셋에서는 검증 과정시 스코어 계산을 위해 OKS(Object Keypoint Similarity)를 사용하는데 기존 coco 키포인트에서 사용한 oks sigma 값을 보고 근사해서 넣은 값과 1로 사용했을 때 결과에서 차이가 없어서 1로 사용했습니다.  \n",
    "학습이 끝난 모델로 테스트 이미지를 추론했을 때 간혹 키포인트가 제대로 나오지 않는 이미지가 발생해서 해당 이미지 발생시 0으로 먼저 채워넣고 다른 모델의 추론 값으로 채워넣는 과정을 거쳤습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "\n",
    "from utils import train_val_split, get_data_dicts, save_samples\n",
    "from Trainer import Trainer\n",
    "\n",
    "\n",
    "def main():\n",
    "    data_name = \"augmented_2\"\n",
    "    data_path = os.path.join(\"./data\", data_name)\n",
    "    csv_name = data_name + \".csv\"\n",
    "    train_df = pd.read_csv(os.path.join(data_path, csv_name))\n",
    "\n",
    "    keypoint_names = list(map(lambda x: x[:-2], train_df.columns.to_list()[1::2]))\n",
    "    keypoint_flip_map = [\n",
    "        (\"left_eye\", \"right_eye\"),\n",
    "        (\"left_ear\", \"right_ear\"),\n",
    "        (\"left_shoulder\", \"right_shoulder\"),\n",
    "        (\"left_elbow\", \"right_elbow\"),\n",
    "        (\"left_wrist\", \"right_wrist\"),\n",
    "        (\"left_hip\", \"right_hip\"),\n",
    "        (\"left_knee\", \"right_knee\"),\n",
    "        (\"left_ankle\", \"right_ankle\"),\n",
    "        (\"left_palm\", \"right_palm\"),\n",
    "        (\"left_instep\", \"right_instep\"),\n",
    "    ]\n",
    "\n",
    "    image_list = train_df.iloc[:, 0].to_numpy()\n",
    "    keypoints_list = train_df.iloc[:, 1:].to_numpy()\n",
    "    train_imgs, valid_imgs, train_keypoints, valid_keypoints = train_val_split(image_list, keypoints_list, random_state=42)\n",
    "\n",
    "    image_set = {\"train\": train_imgs, \"valid\": valid_imgs}\n",
    "    keypoints_set = {\"train\": train_keypoints, \"valid\": valid_keypoints}\n",
    "\n",
    "    hyper_params = {\n",
    "        \"augmented_ver\": data_name,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"num_epochs\": 10000,\n",
    "        \"batch_size\": 256,\n",
    "        \"description\": \"Final training\"\n",
    "    }\n",
    "\n",
    "    for phase in [\"train\", \"valid\"]:\n",
    "        DatasetCatalog.register(\n",
    "            \"keypoints_\" + phase, lambda phase=phase: get_data_dicts(data_path, image_set[phase], keypoints_set[phase])\n",
    "        )\n",
    "        MetadataCatalog.get(\"keypoints_\" + phase).set(thing_classes=[\"human\"])\n",
    "        MetadataCatalog.get(\"keypoints_\" + phase).set(keypoint_names=keypoint_names)\n",
    "        MetadataCatalog.get(\"keypoints_\" + phase).set(keypoint_flip_map=keypoint_flip_map)\n",
    "        MetadataCatalog.get(\"keypoints_\" + phase).set(evaluator_type=\"coco\")\n",
    "\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
    "    cfg.DATASETS.TRAIN = (\"keypoints_train\",)\n",
    "    cfg.DATASETS.TEST = (\"keypoints_valid\",)\n",
    "    cfg.DATALOADER.NUM_WORKERS = 16  # On Windows environment, this value must be 0.\n",
    "    cfg.SOLVER.IMS_PER_BATCH = 2  # mini batch size would be (SOLVER.IMS_PER_BATCH) * (ROI_HEADS.BATCH_SIZE_PER_IMAGE).\n",
    "    cfg.SOLVER.BASE_LR = hyper_params[\"learning_rate\"]  # Learning Rate.\n",
    "    cfg.SOLVER.MAX_ITER = hyper_params[\"num_epochs\"]  # Max iteration.\n",
    "    cfg.SOLVER.GAMMA = 0.8\n",
    "    cfg.SOLVER.STEPS = [3000, 4000, 5000, 6000, 7000, 8000]  # The iteration number to decrease learning rate by GAMMA.\n",
    "    # cfg.SOLVER.LR_SCHEDULER_NAME = \"WarmupMultiStepLR\"\n",
    "    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Keypoints/keypoint_rcnn_X_101_32x8d_FPN_3x.yaml\")\n",
    "    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = hyper_params[\"batch_size\"]  # Use to calculate RPN loss.\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 24\n",
    "    cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((24, 1), dtype=float).tolist()\n",
    "    cfg.TEST.EVAL_PERIOD = 5000  # Evaluation would occur for every cfg.TEST.EVAL_PERIOD value.\n",
    "    cfg.OUTPUT_DIR = os.path.join(\"./output\", data_name)\n",
    "\n",
    "    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()\n",
    "\n",
    "    # Inference should use the config with parameters that are used in training\n",
    "    # cfg now already contains everything we've set previously. We changed it a little bit for inference:\n",
    "    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # set a custom testing threshold\n",
    "    predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    test_dir = os.path.join(\"data\", \"test_imgs\")\n",
    "    test_list = os.listdir(test_dir)\n",
    "    test_list.sort()\n",
    "    except_list = []\n",
    "\n",
    "    files = []\n",
    "    preds = []\n",
    "    for file in tqdm(test_list):\n",
    "        filepath = os.path.join(test_dir, file)\n",
    "        # print(filepath)\n",
    "        im = cv2.imread(filepath)\n",
    "        outputs = predictor(im)\n",
    "        outputs = outputs[\"instances\"].to(\"cpu\").get(\"pred_keypoints\").numpy()\n",
    "        files.append(file)\n",
    "        pred = []\n",
    "        try:\n",
    "            for out in outputs[0]:\n",
    "                pred.extend([float(e) for e in out[:2]])\n",
    "        except IndexError:\n",
    "            pred.extend([0] * 48)\n",
    "            except_list.append(filepath)\n",
    "        preds.append(pred)\n",
    "\n",
    "    df_sub = pd.read_csv(\"./data/sample_submission.csv\")\n",
    "    df = pd.DataFrame(columns=df_sub.columns)\n",
    "    df[\"image\"] = files\n",
    "    df.iloc[:, 1:] = preds\n",
    "\n",
    "    df.to_csv(os.path.join(cfg.OUTPUT_DIR, f\"{data_name}_submission.csv\"), index=False)\n",
    "    if except_list:\n",
    "        print(\n",
    "            \"The following images are not detected keypoints. The row corresponding that images names would be filled with 0 value.\"\n",
    "        )\n",
    "        print(*except_list)\n",
    "    save_samples(cfg.OUTPUT_DIR, test_dir, os.path.join(cfg.OUTPUT_DIR, f\"{data_name}_submission.csv\"), mode=\"random\", size=5)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
