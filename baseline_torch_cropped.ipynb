{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data as data_utils\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import Models\n",
    "\n",
    "# Connect your script to Neptune\n",
    "import neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix data directory\n",
    "prefix_dir = '.'\n",
    "\n",
    "env = 'lk3'\n",
    "\n",
    "# Use Yolo\n",
    "use_yolo = True\n",
    "cropped = 'cropped2_' if use_yolo else ''\n",
    "\n",
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "# to the ImageFolder structure\n",
    "train_dir = f'{prefix_dir}/data/{cropped}train_imgs'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = 'resnext50_32x4d'\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 48\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 64\n",
    "\n",
    "# Number of epochs and earlystop to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# validation set ratio\n",
    "num_splits = 10\n",
    "num_earlystop = 20 if num_epochs // 10 < 20 else num_epochs // 10\n",
    "# not use\n",
    "num_earlystop = 0\n",
    "\n",
    "# Iput size for resize imgae\n",
    "input_size = 128\n",
    "\n",
    "# Learning rate for optimizer\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Use K-folds\n",
    "use_kfolds = False\n",
    "\n",
    "# Use multi-GPU\n",
    "cuda_num = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{prefix_dir}/data/pre-processed/{cropped}train_df.csv')\n",
    "\n",
    "imgs = df.iloc[:, 0].to_numpy()\n",
    "motions = df.iloc[:, 1:]\n",
    "columns = motions.columns.to_list()[::2]\n",
    "class_labels = [label.replace('_x', '').replace('_y', '') for label in columns]\n",
    "keypoints = []\n",
    "for motion in motions.to_numpy():\n",
    "    a_keypoints = []\n",
    "    for i in range(0, motion.shape[0], 2):\n",
    "        a_keypoints.append((float(motion[i]), float(motion[i+1])))\n",
    "    keypoints.append(a_keypoints)\n",
    "keypoints = np.array(keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'001-1-1-01-Z17_A-0000001.jpg'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0011101Z17_A'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(imgs[0].split(\"-\")[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1], dtype=int64),)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(imgs == imgs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(imgs, keypoints, random_state):\n",
    "    d = dict()\n",
    "    for file in imgs:\n",
    "        key = \"\".join(file.split(\"-\")[:-1])\n",
    "        if key not in d.keys():\n",
    "            d[key] = [file]\n",
    "        else:\n",
    "            d[key].append(file)\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "    trains = []\n",
    "    validations = []\n",
    "    for key, value in d.items():\n",
    "        r = np.random.randint(len(value), size=2)\n",
    "        for i in range(len(value)):\n",
    "            if i in r:\n",
    "                validations.append(np.where(imgs == value[i])[0][0])\n",
    "            else:\n",
    "                trains.append(np.where(imgs == value[i])[0][0])\n",
    "    return imgs[trains], imgs[validations], keypoints[trains], keypoints[validations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs, valid_imgs, train_keypoints, valid_keypoints = train_val_split(imgs, keypoints, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['001-1-1-01-Z17_A-0000001.jpg', '001-1-1-01-Z17_A-0000003.jpg',\n",
       "       '001-1-1-01-Z17_A-0000005.jpg', ...,\n",
       "       '642-2-4-31-Z148_E-0000027.jpg', '642-2-4-31-Z148_E-0000029.jpg',\n",
       "       '642-2-4-31-Z148_E-0000031.jpg'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3679,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3679, 24, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_keypoints.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(516, 24, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_keypoints.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/mybirth0407/dacon-motion/e/DAC-212\n"
     ]
    }
   ],
   "source": [
    "ns = neptune.init(project_qualified_name='mybirth0407/dacon-motion',\n",
    "             api_token=neptune_config.token)\n",
    "\n",
    "# Create experiment\n",
    "neptune.create_experiment(f'{model_name}')\n",
    "\n",
    "neptune.log_metric('batch_size', batch_size)\n",
    "neptune.log_metric('num_epochs', num_epochs)\n",
    "neptune.log_metric('num_splits', num_splits)\n",
    "neptune.log_metric('num_ealrystop', num_earlystop)\n",
    "neptune.log_metric('input_size', input_size)\n",
    "neptune.log_metric('learning_rate', learning_rate)\n",
    "neptune.log_metric('use_kfolds', use_kfolds)\n",
    "neptune.log_metric('use_yolo', use_yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAC-212\n"
     ]
    }
   ],
   "source": [
    "counter = ns._get_current_experiment()._id\n",
    "os.mkdir(f'{prefix_dir}/{env}/{counter}')\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, earlystop=0, num_epochs=25, monitor='val', allsave=False, phases=['train', 'val']):\n",
    "    since = time.time()\n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    earlystop_value = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 999999999\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_since = time.time()\n",
    "        if earlystop and earlystop_value >= earlystop:\n",
    "            break\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in phases:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs.float(), labels.float())\n",
    "\n",
    "                    # for classification\n",
    "#                     _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # for classification\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "                # for regression\n",
    "#                 running_corrects += torch.sum(outputs.float() == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            epoch_time_elapsed = time.time() - epoch_since\n",
    "            print('{} ({}) Loss: {:.4f} Elapsed time: {:.0f}m {:.0f}s'.format(\n",
    "                phase, len(dataloaders[phase].dataset), epoch_loss, epoch_time_elapsed // 60, epoch_time_elapsed % 60))\n",
    "            neptune.log_metric(f'{phase}_loss', epoch_loss)\n",
    "#             neptune.log_metric(f'{phase}_acc', epoch_acc)\n",
    "            \n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if monitor == 'val':\n",
    "                    if epoch_loss < best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                        neptune.log_metric(f'{phase}_best_loss', best_loss)\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}.pt')\n",
    "                        print('copied model')\n",
    "                        earlystop_value = 0\n",
    "                    else:\n",
    "                        earlystop_value += 1\n",
    "                        if allsave:\n",
    "                            torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}_{epoch_loss:.2f}_{epoch}.pt')\n",
    "                    val_loss_history.append(epoch_loss)\n",
    "            elif phase == 'train':\n",
    "                if monitor == 'train':\n",
    "                    if epoch_loss < best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                        neptune.log_metric(f'{phase}_best_loss', best_loss)\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}.pt')\n",
    "                        print('copied model')\n",
    "                        earlystop_value = 0\n",
    "                    else:\n",
    "                        earlystop_value += 1\n",
    "                        if allsave:\n",
    "                            torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}_{epoch_loss:.2f}_{epoch}.pt')\n",
    "                    train_loss_history.append(epoch_loss)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training and Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    if monitor == 'val':\n",
    "        print('Best Validation Loss: {:4f}\\n'.format(best_loss))\n",
    "    elif monitor == 'train':\n",
    "        print('Best Training Loss: {:4f}\\n'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the model for this run\n",
    "model_ft, input_size = model.initialize_model(model_name, input_size, num_classes, use_pretrained=True)\n",
    "# set_parameter_requires_grad(model_ft, feature_extract)\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(f'cuda:{cuda_num}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Multi GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2, 3'\n",
    "model = nn.DataParallel(model_ft, device_ids=[2, 3], output_device=2)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data augmentation and normalization for training\n",
    "# # Just resize and normalization for validation\n",
    "\n",
    "A_transforms = {\n",
    "    'train':\n",
    "        A.Compose([\n",
    "            A.Resize(input_size, input_size, always_apply=True),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "#             A.HorizontalFlip(p=0.3),\n",
    "            A.OneOf([\n",
    "                A.RandomRotate90(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "            ], p=0.5),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(p=1),\n",
    "                A.GaussNoise(p=1)                 \n",
    "            ], p=0.5),\n",
    "            \n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels'], remove_invisible=False, angle_in_degrees=True)),\n",
    "    \n",
    "    'val':\n",
    "        A.Compose([\n",
    "            A.Resize(input_size, input_size, always_apply=True),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', label_fields=['class_labels'], remove_invisible=False, angle_in_degrees=True)),\n",
    "    \n",
    "    'test':\n",
    "        A.Compose([\n",
    "            A.Resize(input_size, input_size, always_apply=True),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data_utils.Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, keypoints, phase, class_labels=None, data_transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.keypoints = keypoints\n",
    "        self.phase = phase\n",
    "        self.class_labels = class_labels\n",
    "        self.data_transforms = data_transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
    "#         img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        keypoints = self.keypoints[idx]\n",
    "    \n",
    "        if self.data_transforms:\n",
    "            augmented = self.data_transforms[self.phase](image=img, keypoints=keypoints, class_labels=self.class_labels)\n",
    "            img = augmented['image']\n",
    "            keypoints = augmented['keypoints']\n",
    "        keypoints = np.array(keypoints).flatten()\n",
    "\n",
    "        return img, keypoints\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-folds use: False\n",
      "yolo use: True\n",
      "Epoch 1/200\n",
      "----------\n",
      "train (3775) Loss: 892.9218 Elapsed time: 0m 21s\n",
      "val (420) Loss: 18581.3934 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 2/200\n",
      "----------\n",
      "train (3775) Loss: 599.3248 Elapsed time: 0m 21s\n",
      "val (420) Loss: 482.8133 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 3/200\n",
      "----------\n",
      "train (3775) Loss: 433.3855 Elapsed time: 0m 21s\n",
      "val (420) Loss: 300.7220 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 4/200\n",
      "----------\n",
      "train (3775) Loss: 336.1645 Elapsed time: 0m 21s\n",
      "val (420) Loss: 189.1350 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 5/200\n",
      "----------\n",
      "train (3775) Loss: 282.6212 Elapsed time: 0m 21s\n",
      "val (420) Loss: 414.8952 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 6/200\n",
      "----------\n",
      "train (3775) Loss: 246.4367 Elapsed time: 0m 21s\n",
      "val (420) Loss: 204.6039 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 7/200\n",
      "----------\n",
      "train (3775) Loss: 191.0396 Elapsed time: 0m 21s\n",
      "val (420) Loss: 123.8226 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 8/200\n",
      "----------\n",
      "train (3775) Loss: 167.8664 Elapsed time: 0m 21s\n",
      "val (420) Loss: 127.8428 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 9/200\n",
      "----------\n",
      "train (3775) Loss: 150.5482 Elapsed time: 0m 21s\n",
      "val (420) Loss: 129.0059 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 10/200\n",
      "----------\n",
      "train (3775) Loss: 139.7420 Elapsed time: 0m 21s\n",
      "val (420) Loss: 91.6795 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 11/200\n",
      "----------\n",
      "train (3775) Loss: 118.2658 Elapsed time: 0m 21s\n",
      "val (420) Loss: 76.6631 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 12/200\n",
      "----------\n",
      "train (3775) Loss: 102.6237 Elapsed time: 0m 21s\n",
      "val (420) Loss: 70.4301 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 13/200\n",
      "----------\n",
      "train (3775) Loss: 97.5762 Elapsed time: 0m 21s\n",
      "val (420) Loss: 71.1085 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 14/200\n",
      "----------\n",
      "train (3775) Loss: 86.1073 Elapsed time: 0m 21s\n",
      "val (420) Loss: 50.8471 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 15/200\n",
      "----------\n",
      "train (3775) Loss: 78.5759 Elapsed time: 0m 21s\n",
      "val (420) Loss: 44.2905 Elapsed time: 0m 23s\n",
      "copied model\n",
      "\n",
      "Epoch 16/200\n",
      "----------\n",
      "train (3775) Loss: 72.0078 Elapsed time: 0m 21s\n",
      "val (420) Loss: 46.6301 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 17/200\n",
      "----------\n",
      "train (3775) Loss: 74.1547 Elapsed time: 0m 21s\n",
      "val (420) Loss: 65.2533 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 18/200\n",
      "----------\n",
      "train (3775) Loss: 75.4188 Elapsed time: 0m 22s\n",
      "val (420) Loss: 37.2400 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 19/200\n",
      "----------\n",
      "train (3775) Loss: 58.3265 Elapsed time: 0m 21s\n",
      "val (420) Loss: 39.0271 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 20/200\n",
      "----------\n",
      "train (3775) Loss: 55.7482 Elapsed time: 0m 22s\n",
      "val (420) Loss: 38.1213 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 21/200\n",
      "----------\n",
      "train (3775) Loss: 51.9440 Elapsed time: 0m 22s\n",
      "val (420) Loss: 29.9633 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 22/200\n",
      "----------\n",
      "train (3775) Loss: 52.3424 Elapsed time: 0m 22s\n",
      "val (420) Loss: 30.8735 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 23/200\n",
      "----------\n",
      "train (3775) Loss: 47.7868 Elapsed time: 0m 22s\n",
      "val (420) Loss: 28.4270 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 24/200\n",
      "----------\n",
      "train (3775) Loss: 45.1760 Elapsed time: 0m 22s\n",
      "val (420) Loss: 32.9600 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 25/200\n",
      "----------\n",
      "train (3775) Loss: 43.8971 Elapsed time: 0m 22s\n",
      "val (420) Loss: 29.9873 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 26/200\n",
      "----------\n",
      "train (3775) Loss: 43.1593 Elapsed time: 0m 22s\n",
      "val (420) Loss: 27.8755 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 27/200\n",
      "----------\n",
      "train (3775) Loss: 46.1317 Elapsed time: 0m 22s\n",
      "val (420) Loss: 26.5924 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 28/200\n",
      "----------\n",
      "train (3775) Loss: 43.9028 Elapsed time: 0m 22s\n",
      "val (420) Loss: 31.4271 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 29/200\n",
      "----------\n",
      "train (3775) Loss: 42.0425 Elapsed time: 0m 22s\n",
      "val (420) Loss: 26.8742 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 30/200\n",
      "----------\n",
      "train (3775) Loss: 40.1291 Elapsed time: 0m 22s\n",
      "val (420) Loss: 31.9000 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 31/200\n",
      "----------\n",
      "train (3775) Loss: 36.0260 Elapsed time: 0m 22s\n",
      "val (420) Loss: 21.6860 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 32/200\n",
      "----------\n",
      "train (3775) Loss: 36.0940 Elapsed time: 0m 22s\n",
      "val (420) Loss: 32.6383 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 33/200\n",
      "----------\n",
      "train (3775) Loss: 38.2593 Elapsed time: 0m 22s\n",
      "val (420) Loss: 22.2344 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 34/200\n",
      "----------\n",
      "train (3775) Loss: 36.3515 Elapsed time: 0m 22s\n",
      "val (420) Loss: 28.2346 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 35/200\n",
      "----------\n",
      "train (3775) Loss: 30.7683 Elapsed time: 0m 22s\n",
      "val (420) Loss: 20.5625 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 36/200\n",
      "----------\n",
      "train (3775) Loss: 32.1285 Elapsed time: 0m 22s\n",
      "val (420) Loss: 21.1463 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 37/200\n",
      "----------\n",
      "train (3775) Loss: 29.1523 Elapsed time: 0m 22s\n",
      "val (420) Loss: 18.8960 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 38/200\n",
      "----------\n",
      "train (3775) Loss: 29.1622 Elapsed time: 0m 22s\n",
      "val (420) Loss: 20.6216 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 39/200\n",
      "----------\n",
      "train (3775) Loss: 29.6620 Elapsed time: 0m 22s\n",
      "val (420) Loss: 22.9730 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 40/200\n",
      "----------\n",
      "train (3775) Loss: 27.7876 Elapsed time: 0m 22s\n",
      "val (420) Loss: 18.1961 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 41/200\n",
      "----------\n",
      "train (3775) Loss: 27.9426 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.1001 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 42/200\n",
      "----------\n",
      "train (3775) Loss: 27.9091 Elapsed time: 0m 22s\n",
      "val (420) Loss: 19.3893 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 43/200\n",
      "----------\n",
      "train (3775) Loss: 27.3071 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.2115 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 44/200\n",
      "----------\n",
      "train (3775) Loss: 25.4123 Elapsed time: 0m 22s\n",
      "val (420) Loss: 22.0410 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 45/200\n",
      "----------\n",
      "train (3775) Loss: 24.9249 Elapsed time: 0m 22s\n",
      "val (420) Loss: 17.8088 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 46/200\n",
      "----------\n",
      "train (3775) Loss: 24.3635 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.4465 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 47/200\n",
      "----------\n",
      "train (3775) Loss: 24.8580 Elapsed time: 0m 22s\n",
      "val (420) Loss: 17.9897 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 48/200\n",
      "----------\n",
      "train (3775) Loss: 24.9569 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.7159 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 49/200\n",
      "----------\n",
      "train (3775) Loss: 24.3635 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.3149 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 50/200\n",
      "----------\n",
      "train (3775) Loss: 24.6962 Elapsed time: 0m 22s\n",
      "val (420) Loss: 18.2349 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 51/200\n",
      "----------\n",
      "train (3775) Loss: 23.8248 Elapsed time: 0m 22s\n",
      "val (420) Loss: 42.3507 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 52/200\n",
      "----------\n",
      "train (3775) Loss: 22.5652 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.9387 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 53/200\n",
      "----------\n",
      "train (3775) Loss: 22.9329 Elapsed time: 0m 22s\n",
      "val (420) Loss: 18.9081 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 54/200\n",
      "----------\n",
      "train (3775) Loss: 20.6846 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.9217 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 55/200\n",
      "----------\n",
      "train (3775) Loss: 23.3253 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.9505 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 56/200\n",
      "----------\n",
      "train (3775) Loss: 20.4764 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.4067 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 57/200\n",
      "----------\n",
      "train (3775) Loss: 20.5741 Elapsed time: 0m 22s\n",
      "val (420) Loss: 41.9116 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 58/200\n",
      "----------\n",
      "train (3775) Loss: 23.4737 Elapsed time: 0m 22s\n",
      "val (420) Loss: 43.2836 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 59/200\n",
      "----------\n",
      "train (3775) Loss: 41.1069 Elapsed time: 0m 22s\n",
      "val (420) Loss: 81.7657 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 60/200\n",
      "----------\n",
      "train (3775) Loss: 43.2358 Elapsed time: 0m 22s\n",
      "val (420) Loss: 107.6631 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 61/200\n",
      "----------\n",
      "train (3775) Loss: 62.4342 Elapsed time: 0m 22s\n",
      "val (420) Loss: 83.1335 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 62/200\n",
      "----------\n",
      "train (3775) Loss: 44.7753 Elapsed time: 0m 22s\n",
      "val (420) Loss: 24.4736 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 63/200\n",
      "----------\n",
      "train (3775) Loss: 28.9336 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.0192 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 64/200\n",
      "----------\n",
      "train (3775) Loss: 23.4206 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.3836 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 65/200\n",
      "----------\n",
      "train (3775) Loss: 22.4679 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.0957 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 66/200\n",
      "----------\n",
      "train (3775) Loss: 20.2445 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.5285 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 67/200\n",
      "----------\n",
      "train (3775) Loss: 18.5085 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.9375 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 68/200\n",
      "----------\n",
      "train (3775) Loss: 17.3383 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.6086 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 69/200\n",
      "----------\n",
      "train (3775) Loss: 17.8420 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.9584 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 70/200\n",
      "----------\n",
      "train (3775) Loss: 17.4336 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.3833 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 71/200\n",
      "----------\n",
      "train (3775) Loss: 17.6113 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.8259 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 72/200\n",
      "----------\n",
      "train (3775) Loss: 17.4492 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.9942 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 73/200\n",
      "----------\n",
      "train (3775) Loss: 18.8356 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.7270 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 74/200\n",
      "----------\n",
      "train (3775) Loss: 18.2436 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.3273 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 75/200\n",
      "----------\n",
      "train (3775) Loss: 16.5843 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.3721 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 76/200\n",
      "----------\n",
      "train (3775) Loss: 15.6036 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.8694 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 77/200\n",
      "----------\n",
      "train (3775) Loss: 16.1550 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.1602 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 78/200\n",
      "----------\n",
      "train (3775) Loss: 16.1605 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.7576 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 79/200\n",
      "----------\n",
      "train (3775) Loss: 16.9885 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.5108 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 80/200\n",
      "----------\n",
      "train (3775) Loss: 15.5443 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.4069 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 81/200\n",
      "----------\n",
      "train (3775) Loss: 14.7954 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.3312 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 82/200\n",
      "----------\n",
      "train (3775) Loss: 14.1477 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.9505 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 83/200\n",
      "----------\n",
      "train (3775) Loss: 14.0474 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.1895 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 84/200\n",
      "----------\n",
      "train (3775) Loss: 15.1714 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.9403 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 85/200\n",
      "----------\n",
      "train (3775) Loss: 13.9526 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.1836 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 86/200\n",
      "----------\n",
      "train (3775) Loss: 13.8449 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.3673 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 87/200\n",
      "----------\n",
      "train (3775) Loss: 14.4010 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.1789 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 88/200\n",
      "----------\n",
      "train (3775) Loss: 13.8226 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.1810 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 89/200\n",
      "----------\n",
      "train (3775) Loss: 13.8558 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.0282 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 90/200\n",
      "----------\n",
      "train (3775) Loss: 15.1716 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.5443 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 91/200\n",
      "----------\n",
      "train (3775) Loss: 13.8874 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.7538 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 92/200\n",
      "----------\n",
      "train (3775) Loss: 13.9228 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.0018 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 93/200\n",
      "----------\n",
      "train (3775) Loss: 13.0378 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.1662 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 94/200\n",
      "----------\n",
      "train (3775) Loss: 15.6743 Elapsed time: 0m 22s\n",
      "val (420) Loss: 19.6046 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 95/200\n",
      "----------\n",
      "train (3775) Loss: 18.2413 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.5819 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 96/200\n",
      "----------\n",
      "train (3775) Loss: 14.1653 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.4958 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 97/200\n",
      "----------\n",
      "train (3775) Loss: 13.8154 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.2496 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 98/200\n",
      "----------\n",
      "train (3775) Loss: 14.6638 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.6323 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 99/200\n",
      "----------\n",
      "train (3775) Loss: 13.7109 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.4992 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 100/200\n",
      "----------\n",
      "train (3775) Loss: 13.1277 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.2467 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 101/200\n",
      "----------\n",
      "train (3775) Loss: 12.9619 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.0244 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 102/200\n",
      "----------\n",
      "train (3775) Loss: 12.6918 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.1941 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 103/200\n",
      "----------\n",
      "train (3775) Loss: 10.9156 Elapsed time: 0m 22s\n",
      "val (420) Loss: 9.8873 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 104/200\n",
      "----------\n",
      "train (3775) Loss: 11.2483 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.4557 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 105/200\n",
      "----------\n",
      "train (3775) Loss: 11.8998 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.2180 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 106/200\n",
      "----------\n",
      "train (3775) Loss: 14.4237 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.9108 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 107/200\n",
      "----------\n",
      "train (3775) Loss: 12.0645 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.2857 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 108/200\n",
      "----------\n",
      "train (3775) Loss: 12.4919 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.7277 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 109/200\n",
      "----------\n",
      "train (3775) Loss: 11.1879 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.8908 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 110/200\n",
      "----------\n",
      "train (3775) Loss: 11.0018 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.2534 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 111/200\n",
      "----------\n",
      "train (3775) Loss: 12.5319 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.7798 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 112/200\n",
      "----------\n",
      "train (3775) Loss: 11.3363 Elapsed time: 0m 22s\n",
      "val (420) Loss: 19.2289 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 113/200\n",
      "----------\n",
      "train (3775) Loss: 13.7300 Elapsed time: 0m 22s\n",
      "val (420) Loss: 17.7539 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 114/200\n",
      "----------\n",
      "train (3775) Loss: 13.8684 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.4172 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 115/200\n",
      "----------\n",
      "train (3775) Loss: 13.7782 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.2657 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 116/200\n",
      "----------\n",
      "train (3775) Loss: 13.1568 Elapsed time: 0m 22s\n",
      "val (420) Loss: 17.7932 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 117/200\n",
      "----------\n",
      "train (3775) Loss: 12.3834 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.9936 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 118/200\n",
      "----------\n",
      "train (3775) Loss: 12.4126 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.3793 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 119/200\n",
      "----------\n",
      "train (3775) Loss: 11.2690 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.8291 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 120/200\n",
      "----------\n",
      "train (3775) Loss: 10.4252 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.7587 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 121/200\n",
      "----------\n",
      "train (3775) Loss: 10.4770 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.8790 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 122/200\n",
      "----------\n",
      "train (3775) Loss: 9.6844 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.0140 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 123/200\n",
      "----------\n",
      "train (3775) Loss: 11.1160 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.3289 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 124/200\n",
      "----------\n",
      "train (3775) Loss: 12.5058 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.3145 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 125/200\n",
      "----------\n",
      "train (3775) Loss: 11.6183 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.5430 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 126/200\n",
      "----------\n",
      "train (3775) Loss: 12.3297 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.6196 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 127/200\n",
      "----------\n",
      "train (3775) Loss: 11.6459 Elapsed time: 0m 22s\n",
      "val (420) Loss: 18.2652 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 128/200\n",
      "----------\n",
      "train (3775) Loss: 13.1207 Elapsed time: 0m 22s\n",
      "val (420) Loss: 38.5446 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 129/200\n",
      "----------\n",
      "train (3775) Loss: 12.8739 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.0895 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 130/200\n",
      "----------\n",
      "train (3775) Loss: 10.8247 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.4370 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 131/200\n",
      "----------\n",
      "train (3775) Loss: 10.1326 Elapsed time: 0m 22s\n",
      "val (420) Loss: 34.5799 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 132/200\n",
      "----------\n",
      "train (3775) Loss: 10.7043 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.2346 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 133/200\n",
      "----------\n",
      "train (3775) Loss: 11.8663 Elapsed time: 0m 22s\n",
      "val (420) Loss: 20.5731 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 134/200\n",
      "----------\n",
      "train (3775) Loss: 10.8259 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.7405 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 135/200\n",
      "----------\n",
      "train (3775) Loss: 11.0504 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.1864 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 136/200\n",
      "----------\n",
      "train (3775) Loss: 12.0033 Elapsed time: 0m 22s\n",
      "val (420) Loss: 20.0906 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 137/200\n",
      "----------\n",
      "train (3775) Loss: 10.5767 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.5510 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 138/200\n",
      "----------\n",
      "train (3775) Loss: 10.4312 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.6557 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 139/200\n",
      "----------\n",
      "train (3775) Loss: 10.2131 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.1033 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 140/200\n",
      "----------\n",
      "train (3775) Loss: 10.7816 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.2829 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 141/200\n",
      "----------\n",
      "train (3775) Loss: 10.1737 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.4938 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 142/200\n",
      "----------\n",
      "train (3775) Loss: 10.5952 Elapsed time: 0m 22s\n",
      "val (420) Loss: 21.7728 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 143/200\n",
      "----------\n",
      "train (3775) Loss: 9.9406 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.3325 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 144/200\n",
      "----------\n",
      "train (3775) Loss: 10.2495 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.7961 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 145/200\n",
      "----------\n",
      "train (3775) Loss: 10.2360 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.9870 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 146/200\n",
      "----------\n",
      "train (3775) Loss: 9.5645 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.0947 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 147/200\n",
      "----------\n",
      "train (3775) Loss: 9.1263 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.2369 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 148/200\n",
      "----------\n",
      "train (3775) Loss: 10.3678 Elapsed time: 0m 22s\n",
      "val (420) Loss: 21.9389 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 149/200\n",
      "----------\n",
      "train (3775) Loss: 11.2700 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.0292 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 150/200\n",
      "----------\n",
      "train (3775) Loss: 10.9769 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.1214 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 151/200\n",
      "----------\n",
      "train (3775) Loss: 11.2289 Elapsed time: 0m 22s\n",
      "val (420) Loss: 15.6160 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 152/200\n",
      "----------\n",
      "train (3775) Loss: 11.0966 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.4792 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 153/200\n",
      "----------\n",
      "train (3775) Loss: 9.1514 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.2999 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 154/200\n",
      "----------\n",
      "train (3775) Loss: 10.9298 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.6458 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 155/200\n",
      "----------\n",
      "train (3775) Loss: 10.4042 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.7325 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 156/200\n",
      "----------\n",
      "train (3775) Loss: 8.6487 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.4172 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 157/200\n",
      "----------\n",
      "train (3775) Loss: 8.2668 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.3289 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 158/200\n",
      "----------\n",
      "train (3775) Loss: 7.8328 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.0002 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 159/200\n",
      "----------\n",
      "train (3775) Loss: 8.7422 Elapsed time: 0m 22s\n",
      "val (420) Loss: 17.5616 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 160/200\n",
      "----------\n",
      "train (3775) Loss: 10.2024 Elapsed time: 0m 22s\n",
      "val (420) Loss: 21.5132 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 161/200\n",
      "----------\n",
      "train (3775) Loss: 12.4261 Elapsed time: 0m 22s\n",
      "val (420) Loss: 143.2558 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 162/200\n",
      "----------\n",
      "train (3775) Loss: 16.1570 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.3134 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 163/200\n",
      "----------\n",
      "train (3775) Loss: 12.5023 Elapsed time: 0m 22s\n",
      "val (420) Loss: 40.7242 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 164/200\n",
      "----------\n",
      "train (3775) Loss: 11.0045 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.5294 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 165/200\n",
      "----------\n",
      "train (3775) Loss: 9.0485 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.0737 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 166/200\n",
      "----------\n",
      "train (3775) Loss: 7.1501 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.1667 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 167/200\n",
      "----------\n",
      "train (3775) Loss: 8.2068 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.1022 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 168/200\n",
      "----------\n",
      "train (3775) Loss: 9.0573 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.3897 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 169/200\n",
      "----------\n",
      "train (3775) Loss: 7.6787 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.3036 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 170/200\n",
      "----------\n",
      "train (3775) Loss: 8.7429 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.1188 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 171/200\n",
      "----------\n",
      "train (3775) Loss: 8.8410 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.6012 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 172/200\n",
      "----------\n",
      "train (3775) Loss: 9.7130 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.2305 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 173/200\n",
      "----------\n",
      "train (3775) Loss: 14.0299 Elapsed time: 0m 22s\n",
      "val (420) Loss: 29.1274 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 174/200\n",
      "----------\n",
      "train (3775) Loss: 15.5323 Elapsed time: 0m 22s\n",
      "val (420) Loss: 17.4140 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 175/200\n",
      "----------\n",
      "train (3775) Loss: 10.5897 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.3066 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 176/200\n",
      "----------\n",
      "train (3775) Loss: 8.7786 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.2321 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 177/200\n",
      "----------\n",
      "train (3775) Loss: 7.7990 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.5835 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 178/200\n",
      "----------\n",
      "train (3775) Loss: 8.1291 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.3409 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 179/200\n",
      "----------\n",
      "train (3775) Loss: 7.3285 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.9727 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 180/200\n",
      "----------\n",
      "train (3775) Loss: 7.5732 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.6212 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 181/200\n",
      "----------\n",
      "train (3775) Loss: 7.6840 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.2671 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 182/200\n",
      "----------\n",
      "train (3775) Loss: 7.7877 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.8308 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 183/200\n",
      "----------\n",
      "train (3775) Loss: 10.2060 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.1244 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 184/200\n",
      "----------\n",
      "train (3775) Loss: 8.2031 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.2278 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 185/200\n",
      "----------\n",
      "train (3775) Loss: 8.0247 Elapsed time: 0m 22s\n",
      "val (420) Loss: 20.0139 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 186/200\n",
      "----------\n",
      "train (3775) Loss: 9.0375 Elapsed time: 0m 22s\n",
      "val (420) Loss: 14.0187 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 187/200\n",
      "----------\n",
      "train (3775) Loss: 8.0170 Elapsed time: 0m 22s\n",
      "val (420) Loss: 17.2905 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 188/200\n",
      "----------\n",
      "train (3775) Loss: 7.5614 Elapsed time: 0m 22s\n",
      "val (420) Loss: 13.3380 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 189/200\n",
      "----------\n",
      "train (3775) Loss: 7.4706 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.5614 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 190/200\n",
      "----------\n",
      "train (3775) Loss: 6.5404 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.1538 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 191/200\n",
      "----------\n",
      "train (3775) Loss: 7.1011 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.3484 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 192/200\n",
      "----------\n",
      "train (3775) Loss: 7.0452 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.0935 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 193/200\n",
      "----------\n",
      "train (3775) Loss: 6.3239 Elapsed time: 0m 22s\n",
      "val (420) Loss: 9.3534 Elapsed time: 0m 24s\n",
      "copied model\n",
      "\n",
      "Epoch 194/200\n",
      "----------\n",
      "train (3775) Loss: 6.7034 Elapsed time: 0m 22s\n",
      "val (420) Loss: 16.7589 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 195/200\n",
      "----------\n",
      "train (3775) Loss: 7.8738 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.6594 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 196/200\n",
      "----------\n",
      "train (3775) Loss: 6.2807 Elapsed time: 0m 22s\n",
      "val (420) Loss: 9.9585 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 197/200\n",
      "----------\n",
      "train (3775) Loss: 8.4985 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.6994 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 198/200\n",
      "----------\n",
      "train (3775) Loss: 7.7296 Elapsed time: 0m 22s\n",
      "val (420) Loss: 10.6921 Elapsed time: 0m 23s\n",
      "\n",
      "Epoch 199/200\n",
      "----------\n",
      "train (3775) Loss: 6.7891 Elapsed time: 0m 22s\n",
      "val (420) Loss: 11.2886 Elapsed time: 0m 24s\n",
      "\n",
      "Epoch 200/200\n",
      "----------\n",
      "train (3775) Loss: 7.5239 Elapsed time: 0m 22s\n",
      "val (420) Loss: 12.0196 Elapsed time: 0m 24s\n",
      "\n",
      "Training and Validation complete in 79m 27s\n",
      "Best Validation Loss: 9.353392\n",
      "\n",
      "Elapsed time: 79m 27s\n",
      "\n",
      "All process done!\n",
      "Elapsed time: 79m 27s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(f'k-folds use: {use_kfolds}')\n",
    "print(f'yolo use: {use_yolo}')\n",
    "\n",
    "full_since = time.time()\n",
    "\n",
    "if use_kfolds:\n",
    "    kf = KFold(num_splits, random_state=42, shuffle=True)\n",
    "\n",
    "    for i, (train_index, val_index) in enumerate(kf.split(imgs)):\n",
    "        print(f'{i+1}/{num_splits} folds iteration')\n",
    "        since = time.time()\n",
    "        X_train, X_val = imgs[train_index], imgs[val_index]\n",
    "        y_train, y_val = keypoints[train_index], keypoints[val_index]\n",
    "        train_data = Dataset(train_dir, X_train, y_train, data_transforms=A_transforms, class_labels=class_labels, phase='train')\n",
    "        val_data = Dataset(train_dir, X_val, y_val, data_transforms=A_transforms, class_labels=class_labels, phase='val')\n",
    "        train_loader = data_utils.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = data_utils.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "        dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "        # Observe that all parameters are being optimized\n",
    "        optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Train and evaluate\n",
    "        model_ft, best_loss = train_model(\n",
    "            model_ft, dataloaders, criterion, optimizer_ft,\n",
    "            num_epochs=num_epochs, earlystop=num_earlystop)\n",
    "        torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}_{i+1}_{best_loss:.2f}.pt')\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Elapsed time: {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "else:\n",
    "    since = time.time()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(imgs, keypoints, test_size=1/num_splits, random_state=42)\n",
    "    train_data = Dataset(train_dir, X_train, y_train, data_transforms=A_transforms, class_labels=class_labels, phase='train')\n",
    "    val_data = Dataset(train_dir, X_val, y_val, data_transforms=A_transforms, class_labels=class_labels, phase='val')\n",
    "    train_loader = data_utils.DataLoader(train_data, batch_size=batch_size, num_workers=8, shuffle=True)\n",
    "    val_loader = data_utils.DataLoader(val_data, batch_size=batch_size, num_workers=8, shuffle=False)\n",
    "    dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train and evaluate\n",
    "    model_ft, best_loss = train_model(\n",
    "        model_ft, dataloaders, criterion, optimizer_ft,\n",
    "        num_epochs=num_epochs, earlystop=num_earlystop)\n",
    "    torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}_{best_loss:.2f}.pt')\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Elapsed time: {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "fulltime_elapsed = time.time() - full_since\n",
    "print('All process done!\\nElapsed time: {:.0f}m {:.0f}s\\n'.format(fulltime_elapsed // 60, fulltime_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ft.load_state_dict(torch.load(f'{prefix_dir}/{env}/{counter}/resnet18_18.95.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = f'./data/{cropped}test_imgs'\n",
    "test_imgs = os.listdir(test_dir)\n",
    "test_imgs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data_utils.Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, phase, data_transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.phase = phase\n",
    "        self.data_transforms = data_transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.imgs[idx]\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]))\n",
    "        h = img.shape[0]\n",
    "        w = img.shape[1]\n",
    "        if self.data_transforms:\n",
    "            augmented = self.data_transforms[self.phase](image=img)\n",
    "            img = augmented['image']\n",
    "        return filename, img, (h, w)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "test_data = TestDataset(test_dir, test_imgs, data_transforms=A_transforms, phase='test')\n",
    "test_loader = data_utils.DataLoader(test_data, batch_size=batch_size * 4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:15<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "files = []\n",
    "shapes = []\n",
    "with torch.no_grad():\n",
    "    for filenames, inputs, shape in tqdm(test_loader):\n",
    "        predictions = list(model_ft(inputs.to(device)).cpu().numpy())\n",
    "        files.extend(filenames)\n",
    "        \n",
    "        shapes.extend(shape)\n",
    "        for prediction in predictions:\n",
    "            all_predictions.append(prediction)\n",
    "            \n",
    "origin_shape_y = shapes[0].numpy()\n",
    "origin_shape_x = shapes[1].numpy()\n",
    "for i in range(1, len(shapes) // 2):\n",
    "    origin_shape_y = np.append(origin_shape_y, shapes[2*i].numpy())\n",
    "    origin_shape_x = np.append(origin_shape_x, shapes[2*i + 1].numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] /= input_size / origin_shape_x[i]\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= input_size / origin_shape_y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(f'{prefix_dir}/data/res2_test_df.csv')\n",
    "res = res_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] += res[i][0]\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] += res[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>nose_x</th>\n",
       "      <th>nose_y</th>\n",
       "      <th>left_eye_x</th>\n",
       "      <th>left_eye_y</th>\n",
       "      <th>right_eye_x</th>\n",
       "      <th>right_eye_y</th>\n",
       "      <th>left_ear_x</th>\n",
       "      <th>left_ear_y</th>\n",
       "      <th>right_ear_x</th>\n",
       "      <th>...</th>\n",
       "      <th>right_palm_x</th>\n",
       "      <th>right_palm_y</th>\n",
       "      <th>spine2(back)_x</th>\n",
       "      <th>spine2(back)_y</th>\n",
       "      <th>spine1(waist)_x</th>\n",
       "      <th>spine1(waist)_y</th>\n",
       "      <th>left_instep_x</th>\n",
       "      <th>left_instep_y</th>\n",
       "      <th>right_instep_x</th>\n",
       "      <th>right_instep_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649-2-4-32-Z148_A-0000001.jpg</td>\n",
       "      <td>1089.400146</td>\n",
       "      <td>526.3927</td>\n",
       "      <td>1085.796021</td>\n",
       "      <td>547.372009</td>\n",
       "      <td>1106.544189</td>\n",
       "      <td>543.517212</td>\n",
       "      <td>1054.99707</td>\n",
       "      <td>591.559448</td>\n",
       "      <td>1123.040039</td>\n",
       "      <td>...</td>\n",
       "      <td>1055.435547</td>\n",
       "      <td>284.209351</td>\n",
       "      <td>955.68866</td>\n",
       "      <td>515.05072</td>\n",
       "      <td>886.791382</td>\n",
       "      <td>485.79895</td>\n",
       "      <td>694.466431</td>\n",
       "      <td>690.139221</td>\n",
       "      <td>831.733643</td>\n",
       "      <td>649.395386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>649-2-4-32-Z148_A-0000003.jpg</td>\n",
       "      <td>1091.523926</td>\n",
       "      <td>526.389648</td>\n",
       "      <td>1087.561279</td>\n",
       "      <td>547.47644</td>\n",
       "      <td>1108.689453</td>\n",
       "      <td>543.553467</td>\n",
       "      <td>1055.715576</td>\n",
       "      <td>590.705017</td>\n",
       "      <td>1124.900635</td>\n",
       "      <td>...</td>\n",
       "      <td>1058.001709</td>\n",
       "      <td>286.759888</td>\n",
       "      <td>955.491089</td>\n",
       "      <td>513.697632</td>\n",
       "      <td>886.659912</td>\n",
       "      <td>484.183716</td>\n",
       "      <td>692.194824</td>\n",
       "      <td>689.83429</td>\n",
       "      <td>832.298584</td>\n",
       "      <td>649.864136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>649-2-4-32-Z148_A-0000005.jpg</td>\n",
       "      <td>1100.746216</td>\n",
       "      <td>520.331299</td>\n",
       "      <td>1098.3479</td>\n",
       "      <td>540.6651</td>\n",
       "      <td>1119.109863</td>\n",
       "      <td>536.957764</td>\n",
       "      <td>1066.727783</td>\n",
       "      <td>584.766602</td>\n",
       "      <td>1139.15686</td>\n",
       "      <td>...</td>\n",
       "      <td>1087.047974</td>\n",
       "      <td>281.664551</td>\n",
       "      <td>969.650391</td>\n",
       "      <td>524.667603</td>\n",
       "      <td>897.210571</td>\n",
       "      <td>500.809143</td>\n",
       "      <td>691.565125</td>\n",
       "      <td>685.203613</td>\n",
       "      <td>839.131531</td>\n",
       "      <td>645.434814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>649-2-4-32-Z148_A-0000007.jpg</td>\n",
       "      <td>1173.5802</td>\n",
       "      <td>541.876221</td>\n",
       "      <td>1179.048462</td>\n",
       "      <td>555.010498</td>\n",
       "      <td>1183.324219</td>\n",
       "      <td>547.131714</td>\n",
       "      <td>1159.481689</td>\n",
       "      <td>592.131714</td>\n",
       "      <td>1173.720215</td>\n",
       "      <td>...</td>\n",
       "      <td>1026.306641</td>\n",
       "      <td>521.795715</td>\n",
       "      <td>1014.225708</td>\n",
       "      <td>579.08667</td>\n",
       "      <td>936.80072</td>\n",
       "      <td>566.228638</td>\n",
       "      <td>705.465698</td>\n",
       "      <td>693.574524</td>\n",
       "      <td>741.414185</td>\n",
       "      <td>636.273438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>649-2-4-32-Z148_A-0000009.jpg</td>\n",
       "      <td>1112.44519</td>\n",
       "      <td>509.738739</td>\n",
       "      <td>1110.798096</td>\n",
       "      <td>531.206665</td>\n",
       "      <td>1130.710693</td>\n",
       "      <td>525.843262</td>\n",
       "      <td>1081.217896</td>\n",
       "      <td>580.119263</td>\n",
       "      <td>1152.450562</td>\n",
       "      <td>...</td>\n",
       "      <td>1144.221924</td>\n",
       "      <td>295.797913</td>\n",
       "      <td>982.062256</td>\n",
       "      <td>532.376587</td>\n",
       "      <td>909.259644</td>\n",
       "      <td>516.29187</td>\n",
       "      <td>684.78064</td>\n",
       "      <td>699.79718</td>\n",
       "      <td>827.391541</td>\n",
       "      <td>657.276123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image       nose_x      nose_y   left_eye_x  \\\n",
       "0  649-2-4-32-Z148_A-0000001.jpg  1089.400146    526.3927  1085.796021   \n",
       "1  649-2-4-32-Z148_A-0000003.jpg  1091.523926  526.389648  1087.561279   \n",
       "2  649-2-4-32-Z148_A-0000005.jpg  1100.746216  520.331299    1098.3479   \n",
       "3  649-2-4-32-Z148_A-0000007.jpg    1173.5802  541.876221  1179.048462   \n",
       "4  649-2-4-32-Z148_A-0000009.jpg   1112.44519  509.738739  1110.798096   \n",
       "\n",
       "   left_eye_y  right_eye_x right_eye_y   left_ear_x  left_ear_y  right_ear_x  \\\n",
       "0  547.372009  1106.544189  543.517212   1054.99707  591.559448  1123.040039   \n",
       "1   547.47644  1108.689453  543.553467  1055.715576  590.705017  1124.900635   \n",
       "2    540.6651  1119.109863  536.957764  1066.727783  584.766602   1139.15686   \n",
       "3  555.010498  1183.324219  547.131714  1159.481689  592.131714  1173.720215   \n",
       "4  531.206665  1130.710693  525.843262  1081.217896  580.119263  1152.450562   \n",
       "\n",
       "   ... right_palm_x right_palm_y spine2(back)_x spine2(back)_y  \\\n",
       "0  ...  1055.435547   284.209351      955.68866      515.05072   \n",
       "1  ...  1058.001709   286.759888     955.491089     513.697632   \n",
       "2  ...  1087.047974   281.664551     969.650391     524.667603   \n",
       "3  ...  1026.306641   521.795715    1014.225708      579.08667   \n",
       "4  ...  1144.221924   295.797913     982.062256     532.376587   \n",
       "\n",
       "  spine1(waist)_x spine1(waist)_y left_instep_x left_instep_y right_instep_x  \\\n",
       "0      886.791382       485.79895    694.466431    690.139221     831.733643   \n",
       "1      886.659912      484.183716    692.194824     689.83429     832.298584   \n",
       "2      897.210571      500.809143    691.565125    685.203613     839.131531   \n",
       "3       936.80072      566.228638    705.465698    693.574524     741.414185   \n",
       "4      909.259644       516.29187     684.78064     699.79718     827.391541   \n",
       "\n",
       "  right_instep_y  \n",
       "0     649.395386  \n",
       "1     649.864136  \n",
       "2     645.434814  \n",
       "3     636.273438  \n",
       "4     657.276123  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.read_csv(f'{prefix_dir}/data/sample_submission.csv')\n",
    "df = pd.DataFrame(columns=df_sub.columns)\n",
    "df['image'] = files\n",
    "df.iloc[:, 1:] = all_predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{prefix_dir}/submissions/{counter}_{model_name}_{best_loss:.2f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tune_train_data = Dataset(train_dir, X_val, y_val, data_transforms=A_transforms, class_labels=class_labels, phase='train')\n",
    "# tune_val_data = Dataset(train_dir, X_train, y_train, data_transforms=A_transforms, class_labels=class_labels, phase='val')\n",
    "# tune_train_loader = data_utils.DataLoader(tune_train_data, batch_size=batch_size, shuffle=True)\n",
    "# tune_val_loader = data_utils.DataLoader(tune_val_data, batch_size=batch_size, shuffle=False)\n",
    "# tune_dataloaders = {'train': tune_train_loader, 'val': tune_val_loader}\n",
    "\n",
    "# # Observe that all parameters are being optimized\n",
    "# optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Train and evaluate\n",
    "# since = time.time()\n",
    "# model_ft, best_loss = train_model(\n",
    "#     model_ft, tune_dataloaders, criterion, optimizer_ft,\n",
    "#     num_epochs=10, earlystop=0, allsave=True)\n",
    "# torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}_tuned_{best_loss:.2f}.pt')\n",
    "# time_elapsed = time.time() - since\n",
    "# print('Elapsed time: {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------\n",
      "train (4195) Loss: 8.8942 Elapsed time: 1m 36s\n",
      "copied model\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n",
      "train (4195) Loss: 8.4387 Elapsed time: 1m 35s\n",
      "copied model\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n",
      "train (4195) Loss: 8.3217 Elapsed time: 1m 35s\n",
      "copied model\n",
      "\n",
      "Training and Validation complete in 4m 47s\n",
      "Best Training Loss: 8.321739\n",
      "\n",
      "Elapsed time: 4m 47s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tune_train_data = Dataset(train_dir, imgs, keypoints, data_transforms=A_transforms, class_labels=class_labels, phase='train')\n",
    "tune_train_loader = data_utils.DataLoader(tune_train_data, batch_size=batch_size, shuffle=True)\n",
    "tune_dataloaders = {'train': tune_train_loader}\n",
    "\n",
    "# Observe that all parmeters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and evaluate\n",
    "since = time.time()\n",
    "model_ft, best_loss = train_model(\n",
    "    model_ft, tune_dataloaders, criterion, optimizer_ft,\n",
    "    num_epochs=3, earlystop=0, allsave=True, monitor='train', phases=['train'])\n",
    "torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter}/{model_name}_tuned_{best_loss:.2f}.pt')\n",
    "time_elapsed = time.time() - since\n",
    "print('Elapsed time: {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 7/7 [00:24<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "files = []\n",
    "shapes = []\n",
    "with torch.no_grad():\n",
    "    for filenames, inputs, shape in tqdm(test_loader):\n",
    "        predictions = list(model_ft(inputs.to(device)).cpu().numpy())\n",
    "        files.extend(filenames)\n",
    "        \n",
    "        shapes.extend(shape)\n",
    "        for prediction in predictions:\n",
    "            all_predictions.append(prediction)\n",
    "            \n",
    "origin_shape_y = shapes[0].numpy()\n",
    "origin_shape_x = shapes[1].numpy()\n",
    "for i in range(1, len(shapes) // 2):\n",
    "    origin_shape_y = np.append(origin_shape_y, shapes[2*i].numpy())\n",
    "    origin_shape_x = np.append(origin_shape_x, shapes[2*i + 1].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] /= input_size / origin_shape_x[i]\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= input_size / origin_shape_y[i]\n",
    "    \n",
    "res_df = pd.read_csv(f'{prefix_dir}/data/res2_test_df.csv')\n",
    "res = res_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] += res[i][0]\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] += res[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>nose_x</th>\n",
       "      <th>nose_y</th>\n",
       "      <th>left_eye_x</th>\n",
       "      <th>left_eye_y</th>\n",
       "      <th>right_eye_x</th>\n",
       "      <th>right_eye_y</th>\n",
       "      <th>left_ear_x</th>\n",
       "      <th>left_ear_y</th>\n",
       "      <th>right_ear_x</th>\n",
       "      <th>...</th>\n",
       "      <th>right_palm_x</th>\n",
       "      <th>right_palm_y</th>\n",
       "      <th>spine2(back)_x</th>\n",
       "      <th>spine2(back)_y</th>\n",
       "      <th>spine1(waist)_x</th>\n",
       "      <th>spine1(waist)_y</th>\n",
       "      <th>left_instep_x</th>\n",
       "      <th>left_instep_y</th>\n",
       "      <th>right_instep_x</th>\n",
       "      <th>right_instep_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649-2-4-32-Z148_A-0000001.jpg</td>\n",
       "      <td>1129.631104</td>\n",
       "      <td>522.625</td>\n",
       "      <td>1131.156006</td>\n",
       "      <td>527.815735</td>\n",
       "      <td>1141.35144</td>\n",
       "      <td>523.064392</td>\n",
       "      <td>1112.727051</td>\n",
       "      <td>544.559082</td>\n",
       "      <td>1138.172241</td>\n",
       "      <td>...</td>\n",
       "      <td>1070.983765</td>\n",
       "      <td>414.300049</td>\n",
       "      <td>996.789185</td>\n",
       "      <td>564.46228</td>\n",
       "      <td>911.210815</td>\n",
       "      <td>561.5979</td>\n",
       "      <td>723.179443</td>\n",
       "      <td>574.711853</td>\n",
       "      <td>787.485657</td>\n",
       "      <td>561.435791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>649-2-4-32-Z148_A-0000003.jpg</td>\n",
       "      <td>1131.684937</td>\n",
       "      <td>523.540344</td>\n",
       "      <td>1133.579102</td>\n",
       "      <td>528.741943</td>\n",
       "      <td>1143.230835</td>\n",
       "      <td>523.817749</td>\n",
       "      <td>1114.635742</td>\n",
       "      <td>545.254272</td>\n",
       "      <td>1138.894043</td>\n",
       "      <td>...</td>\n",
       "      <td>1072.407349</td>\n",
       "      <td>414.599213</td>\n",
       "      <td>996.275146</td>\n",
       "      <td>564.355591</td>\n",
       "      <td>909.294861</td>\n",
       "      <td>561.802063</td>\n",
       "      <td>724.853271</td>\n",
       "      <td>565.891479</td>\n",
       "      <td>785.125732</td>\n",
       "      <td>554.425049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>649-2-4-32-Z148_A-0000005.jpg</td>\n",
       "      <td>1157.23291</td>\n",
       "      <td>519.015991</td>\n",
       "      <td>1159.444092</td>\n",
       "      <td>525.924316</td>\n",
       "      <td>1168.587036</td>\n",
       "      <td>519.792847</td>\n",
       "      <td>1135.520508</td>\n",
       "      <td>547.313599</td>\n",
       "      <td>1157.648315</td>\n",
       "      <td>...</td>\n",
       "      <td>1081.33374</td>\n",
       "      <td>411.499146</td>\n",
       "      <td>1022.998047</td>\n",
       "      <td>578.993042</td>\n",
       "      <td>936.562073</td>\n",
       "      <td>583.102539</td>\n",
       "      <td>712.629761</td>\n",
       "      <td>573.341614</td>\n",
       "      <td>784.264038</td>\n",
       "      <td>549.679993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>649-2-4-32-Z148_A-0000007.jpg</td>\n",
       "      <td>1207.16626</td>\n",
       "      <td>618.785217</td>\n",
       "      <td>1218.654297</td>\n",
       "      <td>615.824829</td>\n",
       "      <td>1216.127075</td>\n",
       "      <td>608.076538</td>\n",
       "      <td>1204.846924</td>\n",
       "      <td>611.371887</td>\n",
       "      <td>1201.344849</td>\n",
       "      <td>...</td>\n",
       "      <td>1129.730103</td>\n",
       "      <td>644.426025</td>\n",
       "      <td>1074.061523</td>\n",
       "      <td>632.277466</td>\n",
       "      <td>979.594849</td>\n",
       "      <td>623.125122</td>\n",
       "      <td>752.833496</td>\n",
       "      <td>550.59845</td>\n",
       "      <td>698.846436</td>\n",
       "      <td>543.408569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>649-2-4-32-Z148_A-0000009.jpg</td>\n",
       "      <td>1169.14209</td>\n",
       "      <td>522.656494</td>\n",
       "      <td>1171.640259</td>\n",
       "      <td>531.883301</td>\n",
       "      <td>1186.068848</td>\n",
       "      <td>522.871887</td>\n",
       "      <td>1144.223633</td>\n",
       "      <td>553.479736</td>\n",
       "      <td>1182.264404</td>\n",
       "      <td>...</td>\n",
       "      <td>1183.747925</td>\n",
       "      <td>432.206146</td>\n",
       "      <td>1031.474243</td>\n",
       "      <td>570.272461</td>\n",
       "      <td>938.329834</td>\n",
       "      <td>579.658813</td>\n",
       "      <td>698.909668</td>\n",
       "      <td>596.77124</td>\n",
       "      <td>787.199219</td>\n",
       "      <td>556.950928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image       nose_x      nose_y   left_eye_x  \\\n",
       "0  649-2-4-32-Z148_A-0000001.jpg  1129.631104     522.625  1131.156006   \n",
       "1  649-2-4-32-Z148_A-0000003.jpg  1131.684937  523.540344  1133.579102   \n",
       "2  649-2-4-32-Z148_A-0000005.jpg   1157.23291  519.015991  1159.444092   \n",
       "3  649-2-4-32-Z148_A-0000007.jpg   1207.16626  618.785217  1218.654297   \n",
       "4  649-2-4-32-Z148_A-0000009.jpg   1169.14209  522.656494  1171.640259   \n",
       "\n",
       "   left_eye_y  right_eye_x right_eye_y   left_ear_x  left_ear_y  right_ear_x  \\\n",
       "0  527.815735   1141.35144  523.064392  1112.727051  544.559082  1138.172241   \n",
       "1  528.741943  1143.230835  523.817749  1114.635742  545.254272  1138.894043   \n",
       "2  525.924316  1168.587036  519.792847  1135.520508  547.313599  1157.648315   \n",
       "3  615.824829  1216.127075  608.076538  1204.846924  611.371887  1201.344849   \n",
       "4  531.883301  1186.068848  522.871887  1144.223633  553.479736  1182.264404   \n",
       "\n",
       "   ... right_palm_x right_palm_y spine2(back)_x spine2(back)_y  \\\n",
       "0  ...  1070.983765   414.300049     996.789185      564.46228   \n",
       "1  ...  1072.407349   414.599213     996.275146     564.355591   \n",
       "2  ...   1081.33374   411.499146    1022.998047     578.993042   \n",
       "3  ...  1129.730103   644.426025    1074.061523     632.277466   \n",
       "4  ...  1183.747925   432.206146    1031.474243     570.272461   \n",
       "\n",
       "  spine1(waist)_x spine1(waist)_y left_instep_x left_instep_y right_instep_x  \\\n",
       "0      911.210815        561.5979    723.179443    574.711853     787.485657   \n",
       "1      909.294861      561.802063    724.853271    565.891479     785.125732   \n",
       "2      936.562073      583.102539    712.629761    573.341614     784.264038   \n",
       "3      979.594849      623.125122    752.833496     550.59845     698.846436   \n",
       "4      938.329834      579.658813    698.909668     596.77124     787.199219   \n",
       "\n",
       "  right_instep_y  \n",
       "0     561.435791  \n",
       "1     554.425049  \n",
       "2     549.679993  \n",
       "3     543.408569  \n",
       "4     556.950928  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub = pd.read_csv(f'{prefix_dir}/data/sample_submission.csv')\n",
    "df = pd.DataFrame(columns=df_sub.columns)\n",
    "df['image'] = files\n",
    "df.iloc[:, 1:] = all_predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{prefix_dir}/submissions/{counter}_{model_name}_tuned_{best_loss:.2f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune.stop()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAC-212\n"
     ]
    }
   ],
   "source": [
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
