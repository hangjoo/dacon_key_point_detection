{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data as data_utils\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.models.detection import KeypointRCNN, backbone_utils, keypointrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops import MultiScaleRoIAlign\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Connect your script to Neptuneimport neptune\n",
    "import neptune\n",
    "import neptune_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefix data directory\n",
    "prefix_dir = '.'\n",
    "\n",
    "env = 'local'\n",
    "\n",
    "# Use Yolo\n",
    "use_yolo = True\n",
    "cropped = 'cropped_' if use_yolo else ''\n",
    "\n",
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "# to the ImageFolder structure\n",
    "train_dir = f'{prefix_dir}/data/{cropped}train_imgs'\n",
    "\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = 'rcnn_resnet18'\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 48\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 4\n",
    "\n",
    "# Number of epochs and earlystop to train for\n",
    "num_epochs = 13\n",
    "\n",
    "num_splits = 10\n",
    "num_earlystop = 20 if num_epochs // 10 < 20 else num_epochs // 10\n",
    "# not use\n",
    "# num_earlystop = 0\n",
    "\n",
    "# Iput size for resize imgae\n",
    "input_size = 384\n",
    "\n",
    "# Learning rate for optimizer\n",
    "learning_rate = 0.003\n",
    "\n",
    "# Use K-folds\n",
    "use_kfolds = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{prefix_dir}/data/{cropped}train_df.csv')\n",
    "\n",
    "imgs = df.iloc[:, 0].to_numpy()\n",
    "motions = df.iloc[:, 1:]\n",
    "columns = motions.columns.to_list()[::2]\n",
    "class_labels = [label.replace('_x', '').replace('_y', '') for label in columns]\n",
    "keypoints = []\n",
    "for motion in motions.to_numpy():\n",
    "    a_keypoints = []\n",
    "    for i in range(0, motion.shape[0], 2):\n",
    "        a_keypoints.append((float(motion[i]), float(motion[i+1])))\n",
    "    keypoints.append(a_keypoints)\n",
    "keypoints = np.array(keypoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "https://ui.neptune.ai/mybirth0407/dacon-motion/e/DAC-160\n"
     ]
    }
   ],
   "source": [
    "neptune.init(project_qualified_name='mybirth0407/dacon-motion',\n",
    "             api_token=neptune_config.token)\n",
    "\n",
    "with open(f'{prefix_dir}/counter.txt', 'r+') as f:\n",
    "    content = f.read().strip()\n",
    "    counter = int(content) + 1\n",
    "    f.seek(0)\n",
    "    f.write(f'{counter}')\n",
    "    print(counter)\n",
    "\n",
    "# Create experiment\n",
    "neptune.create_experiment(f'{counter:3d} - {model_name}')\n",
    "\n",
    "neptune.log_metric('batch_size', batch_size)\n",
    "neptune.log_metric('num_epochs', num_epochs)\n",
    "neptune.log_metric('num_splits', num_splits)\n",
    "neptune.log_metric('num_ealrystop', num_earlystop)\n",
    "neptune.log_metric('input_size', input_size)\n",
    "neptune.log_metric('learning_rate', learning_rate)\n",
    "neptune.log_metric('use_kfolds', use_kfolds)\n",
    "neptune.log_metric('use_yolo', use_yolo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, earlystop=0, num_epochs=25, monitor='val'):\n",
    "    since = time.time()\n",
    "    \n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    \n",
    "    earlystop_value = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 999999999\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_since = time.time()\n",
    "        if earlystop and earlystop_value >= earlystop:\n",
    "            break\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "#         for phase in ['train', 'val']:\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for imgs, targets in dataloaders[phase]:\n",
    "                imgs = [img.to(device) for img in imgs]\n",
    "                targets = [{k: v.to(device) for k, v in target.items()} for target in targets]\n",
    "#                 targets = [{k: v.to(device) for k, v in targets.items()}]\n",
    "#                 boxes = {k: v.to(device) for k, v in boxes.items()}\n",
    "#                 labels = {k: v.to(device) for k, v in labels.items()}\n",
    "#                 keypoints = {k: v.to(device) for k, v in keypoints.items()}\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase=='train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    outputs = model(imgs, targets)\n",
    "                    loss = sum(output for output in outputs.values())\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss\n",
    "                # for classification\n",
    "#                 running_corrects += torch.sum(preds == labels.data)\n",
    "                # for regression\n",
    "#                 running_corrects += torch.sum(outputs == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "#             epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            epoch_time_elapsed = time.time() - epoch_since\n",
    "            print('{} ({}) Loss: {:.4f} Elapsed time: {:.0f}m {:.0f}s'.format(\n",
    "                phase, len(dataloaders[phase].dataset), epoch_loss, epoch_time_elapsed // 60, epoch_time_elapsed % 60))\n",
    "            neptune.log_metric(f'{phase}_loss', epoch_loss)\n",
    "#             neptune.log_metric(f'{phase}_acc', epoch_acc)\n",
    "            \n",
    "                \n",
    "            # deep copy the model\n",
    "            if phase == 'val':\n",
    "                if monitor == 'val':\n",
    "                    if epoch_loss < best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter:3d}_{model_name}.pt')\n",
    "                        print('copied model')\n",
    "                        earlystop_value = 0\n",
    "                    else:\n",
    "                        earlystop_value += 1\n",
    "                    val_loss_history.append(epoch_loss)\n",
    "            elif phase == 'train':\n",
    "                if monitor == 'train':\n",
    "                    if epoch_loss < best_loss:\n",
    "                        best_loss = epoch_loss\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter:3d}_{model_name}.pt')\n",
    "                        print('copied model')\n",
    "                        earlystop_value = 0\n",
    "                    else:\n",
    "                        earlystop_value += 1\n",
    "                    train_loss_history.append(epoch_loss)\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training and Validation complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    if monitor == 'val':\n",
    "        print('Best Validation Loss: {:4f}\\n'.format(best_loss))\n",
    "    elif monitor == 'train':\n",
    "        print('Best Training Loss: {:4f}\\n'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(pretrained=False, progress=True, num_classes=2, num_keypoints=24,\n",
    "                     pretrained_backbone=True, trainable_backbone_layers=None,\n",
    "                     rpn_anchor_generator=None, box_roi_pool=None, keypoint_roi_pool=None):\n",
    "    \n",
    "    backbone = backbone_utils.resnet_fpn_backbone('resnet18', pretrained_backbone, trainable_layers=trainable_backbone_layers)\n",
    "    \n",
    "    model = KeypointRCNN(\n",
    "        backbone, \n",
    "        num_classes=num_classes,\n",
    "        num_keypoints=num_keypoints,\n",
    "        rpn_anchor_generator=rpn_anchor_generator,\n",
    "        box_roi_pool=box_roi_pool,\n",
    "        keypoint_roi_pool=keypoint_roi_pool\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_sizes = ((32,), (64,), (128,), (256,), (512,))\n",
    "# anchor_sizes = ((32,), (64,), (128,))\n",
    "aspect_ratios = ((0.5, 1.0, 2.0),) * len(anchor_sizes)\n",
    "anchor_generator = AnchorGenerator(\n",
    "    anchor_sizes, aspect_ratios\n",
    ")\n",
    "\n",
    "roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0'], output_size=7, sampling_ratio=2\n",
    ")\n",
    "keypoint_roi_pooler = MultiScaleRoIAlign(\n",
    "    featmap_names=['0'], output_size=14, sampling_ratio=2\n",
    ")\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft = initialize_model(\n",
    "    pretrained=False, progress=True, num_classes=2, num_keypoints=24,\n",
    "    pretrained_backbone=True, trainable_backbone_layers=3,\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler, keypoint_roi_pool=keypoint_roi_pooler\n",
    ")\n",
    "\n",
    "for param in model_ft.backbone.fpn.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Send the model to GPU\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data augmentation and normalization for training\n",
    "# # Just resize and normalization for validation\n",
    "\n",
    "A_transforms = {\n",
    "    'train':\n",
    "        A.Compose([\n",
    "            A.Resize(input_size, input_size, always_apply=True),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "#             A.RandomRotate90(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.MotionBlur(p=0.3),\n",
    "            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "            keypoint_params=A.KeypointParams(format='xy')),\n",
    "    \n",
    "    'val':\n",
    "        A.Compose([\n",
    "            A.Resize(input_size, input_size, always_apply=True),\n",
    "#             A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']),\n",
    "            keypoint_params=A.KeypointParams(format='xy')),\n",
    "    \n",
    "    'test':\n",
    "        A.Compose([\n",
    "            A.Resize(input_size, input_size, always_apply=True),\n",
    "#             A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data_utils.Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, keypoints, phase, class_labels=None, data_transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.keypoints = keypoints\n",
    "        self.phase = phase\n",
    "        self.class_labels = class_labels\n",
    "        self.data_transforms = data_transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        keypoints = self.keypoints[idx]\n",
    "        x1, y1 = self.keypoints[idx][:, 0].min(), self.keypoints[idx][:, 1].min()\n",
    "        x2, y2 = self.keypoints[idx][:, 0].max(), self.keypoints[idx][:, 1].max()\n",
    "        bboxes = np.array([[x1, y1, x2, y2]], dtype=int)\n",
    "        labels = np.array([1], dtype=int)\n",
    "        targets = {\n",
    "            'image': img,\n",
    "            'bboxes': bboxes,\n",
    "            'labels': labels, # human is 1, 0 is background\n",
    "            'keypoints': keypoints\n",
    "        }\n",
    "        \n",
    "        if self.data_transforms:\n",
    "            targets = self.data_transforms[self.phase](**targets)\n",
    "            img = targets['image']\n",
    "        \n",
    "        targets = {\n",
    "            'labels': torch.as_tensor(targets['labels'], dtype=torch.int64),\n",
    "            'boxes': torch.as_tensor(targets['bboxes'], dtype=torch.float32),\n",
    "            'keypoints': torch.as_tensor(\n",
    "                np.concatenate([targets['keypoints'], np.ones((24, 1))], axis=1)[np.newaxis], dtype=torch.float32)\n",
    "        }\n",
    "        return img, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch: torch.Tensor) -> Tuple:\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k-folds use: False\n",
      "Epoch 1/13\n",
      "----------\n",
      "train (4195) Loss: 1149991.6250 Elapsed time: 10m 29s\n",
      "copied model\n",
      "\n",
      "Epoch 2/13\n",
      "----------\n",
      "train (4195) Loss: 2.0739 Elapsed time: 10m 47s\n",
      "copied model\n",
      "\n",
      "Epoch 3/13\n",
      "----------\n",
      "train (4195) Loss: 2.0710 Elapsed time: 10m 21s\n",
      "copied model\n",
      "\n",
      "Epoch 4/13\n",
      "----------\n",
      "train (4195) Loss: 2.0714 Elapsed time: 10m 34s\n",
      "\n",
      "Epoch 5/13\n",
      "----------\n",
      "train (4195) Loss: 2.0711 Elapsed time: 10m 9s\n",
      "\n",
      "Epoch 6/13\n",
      "----------\n",
      "train (4195) Loss: 2.0711 Elapsed time: 10m 23s\n",
      "\n",
      "Epoch 7/13\n",
      "----------\n",
      "train (4195) Loss: 2.0717 Elapsed time: 10m 36s\n",
      "\n",
      "Epoch 8/13\n",
      "----------\n",
      "train (4195) Loss: 2.0719 Elapsed time: 10m 42s\n",
      "\n",
      "Epoch 9/13\n",
      "----------\n",
      "train (4195) Loss: 2.0719 Elapsed time: 10m 31s\n",
      "\n",
      "Epoch 10/13\n",
      "----------\n",
      "train (4195) Loss: 2.0713 Elapsed time: 10m 24s\n",
      "\n",
      "Epoch 11/13\n",
      "----------\n",
      "train (4195) Loss: 2.0721 Elapsed time: 10m 16s\n",
      "\n",
      "Epoch 12/13\n",
      "----------\n",
      "train (4195) Loss: 2.0721 Elapsed time: 10m 9s\n",
      "\n",
      "Epoch 13/13\n",
      "----------\n",
      "train (4195) Loss: 2.0716 Elapsed time: 10m 17s\n",
      "\n",
      "Training and Validation complete in 135m 42s\n",
      "Best Training Loss: 2.070960\n",
      "\n",
      "Elapsed time: 135m 48s\n",
      "\n",
      "All process done!\n",
      "Elapsed time: 135m 48s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'k-folds use: {use_kfolds}')\n",
    "\n",
    "full_since = time.time()\n",
    "\n",
    "since = time.time()\n",
    "# X_train, X_val, y_train, y_val = train_test_split(imgs, keypoints, test_size=1/num_splits, random_state=42)\n",
    "train_data = Dataset(train_dir, imgs, keypoints, data_transforms=A_transforms, class_labels=class_labels, phase='train')\n",
    "# val_data = Dataset(train_dir, X_val, y_val, data_transforms=A_transforms, class_labels=class_labels, phase='val')\n",
    "train_loader = data_utils.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "# val_loader = data_utils.DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "# dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "dataloaders = {'train': train_loader}\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=learning_rate)\n",
    "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.02, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, best_loss = train_model(\n",
    "    model_ft, dataloaders, optimizer_ft,\n",
    "    num_epochs=num_epochs, earlystop=num_earlystop, monitor='train')\n",
    "torch.save(model_ft.state_dict(), f'{prefix_dir}/{env}/{counter:3d}_{model_name}_{best_loss:.2f}.pt')\n",
    "time_elapsed = time.time() - since\n",
    "print('Elapsed time: {:.0f}m {:.0f}s\\n'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "fulltime_elapsed = time.time() - full_since\n",
    "print('All process done!\\nElapsed time: {:.0f}m {:.0f}s\\n'.format(fulltime_elapsed // 60, fulltime_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_ft.load_state_dict(torch.load(f'{prefix_dir}/{env}/{counter:3d}_{model_name}_{best_loss:.2f}.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = f'{prefix_dir}/data/{cropped}test_imgs'\n",
    "test_imgs = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(data_utils.Dataset):\n",
    "    \"\"\"__init__ and __len__ functions are the same as in TorchvisionDataset\"\"\"\n",
    "    def __init__(self, data_dir, imgs, phase, data_transforms=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.imgs = imgs\n",
    "        self.phase = phase\n",
    "        self.data_transforms = data_transforms\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.imgs[idx]\n",
    "        # Read an image with OpenCV\n",
    "        img = cv2.imread(os.path.join(self.data_dir, self.imgs[idx]), cv2.COLOR_BGR2RGB)\n",
    "        h = img.shape[0]\n",
    "        w = img.shape[1]\n",
    "        if self.data_transforms:\n",
    "            augmented = self.data_transforms[self.phase](image=img)\n",
    "            img = augmented['image']\n",
    "        return filename, img, (h, w)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "test_data = TestDataset(test_dir, test_imgs, data_transforms=A_transforms, phase='test')\n",
    "test_loader = data_utils.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeypointRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(640, 672, 704, 736, 768, 800), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(128)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign()\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "    (keypoint_roi_pool): MultiScaleRoIAlign()\n",
       "    (keypoint_head): KeypointRCNNHeads(\n",
       "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "    )\n",
       "    (keypoint_predictor): KeypointRCNNPredictor(\n",
       "      (kps_score_lowres): ConvTranspose2d(512, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(os.path.join(test_dir, test_imgs[0]), cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (384, 384))\n",
    "img = img / 255.0\n",
    "img = img.transpose(2, 0, 1)\n",
    "img = [torch.as_tensor(img, dtype=torch.float32).to(device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([], device='cuda:0', size=(0, 4), grad_fn=<StackBackward>), 'labels': tensor([], device='cuda:0', dtype=torch.int64), 'scores': tensor([], device='cuda:0', grad_fn=<IndexBackward>), 'keypoints': tensor([], device='cuda:0', size=(0, 24, 3)), 'keypoints_scores': tensor([], device='cuda:0', size=(0, 24))}]\n"
     ]
    }
   ],
   "source": [
    "model_ft.eval()\n",
    "# with torch.no_grad():\n",
    "print(model_ft(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = []\n",
    "files = []\n",
    "shapes = []\n",
    "model_ft.eval()\n",
    "# with torch.no_grad():\n",
    "for filenames, inputs, shape in tqdm(test_loader):\n",
    "    predictions = model_ft(inputs.to(device))\n",
    "    break\n",
    "#         files.extend(filenames)\n",
    "        \n",
    "#         shapes.extend(shape)\n",
    "#         for prediction in predictions:\n",
    "#             all_predictions.append(prediction)\n",
    "            \n",
    "# origin_shape_y = shapes[0].numpy()\n",
    "# origin_shape_x = shapes[1].numpy()\n",
    "# for i in range(1, len(shapes) // 2):\n",
    "#     origin_shape_y = np.append(origin_shape_y, shapes[2*i].numpy())\n",
    "#     origin_shape_x = np.append(origin_shape_x, shapes[2*i + 1].numpy())\n",
    "\n",
    "# all_predictions = np.array(all_predictions)\n",
    "# for i in range(all_predictions.shape[0]):\n",
    "#     all_predictions[i, [2*j for j in range(num_classes//2)]] /= input_size / origin_shape_x[i]\n",
    "#     all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] /= input_size / origin_shape_y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(f'{prefix_dir}/data/res_test_df.csv')\n",
    "res = res_df.iloc[:, 1:].to_numpy()\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "for i in range(all_predictions.shape[0]):\n",
    "    all_predictions[i, [2*j for j in range(num_classes//2)]] += res[i][0]\n",
    "    all_predictions[i, [2*j + 1 for j in range(num_classes//2)]] += res[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(f'{prefix_dir}/data/sample_submission.csv')\n",
    "df = pd.DataFrame(columns=df_sub.columns)\n",
    "df['image'] = files\n",
    "df.iloc[:, 1:] = all_predictions\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{prefix_dir}/submissions/submission_{counter:3d}_{model_name}{model_ver}_{best_loss:.2f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(counter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_90 (Python 3.7)",
   "language": "python",
   "name": "torch_90"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
